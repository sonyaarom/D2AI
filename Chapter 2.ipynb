{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Preliminaries\n",
    "## 2.1 Data Manipulation\n",
    "### General Libraries Needed for the Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import hashlib\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "d2l = sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoking tf.range, we get evenly spaced values, starting with 0 (included) and ending at n (not included, our case - 12). By default, the interval size - [1].\n",
    "Unless otherwise specified, the tensors are stored in the main computer memory and designated for CPU - based computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 12:14:09.814238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(12, dtype=tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access tensor shape and size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=12>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "tf.size(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reshape the tensor without altering the number of elements or their values, use tf.reshape(). In our case, the vector of 12 values becomes the matrix of 3 rows and 4 columns.\n",
    "We can also not specify the other dimension, if we aware of at least one. For example, we do not need to specify the number of columns if we know the number of rows as tensor is capable of figuring it out. Check how in the example of matrix y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(x, (3, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create several zero matrices or other variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros((2, 3, 4)) #shape: 2 matrices, 3 rows each, 4 columns each\n",
    "tf.ones((2, 3, 4)) #shape: 2 matrices, 3 rows each, 4 columns each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet creates a tensor with shape (3, 4). Each of its elements is randomly sampled from a standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1. Useful for randomly  sampling the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-0.9325234 , -0.12558588,  0.50218314,  0.7598414 ],\n",
       "       [-0.89064795, -0.00451622, -0.5613084 ,  0.75285846],\n",
       "       [ 0.50644535,  0.38225195,  0.02749106,  0.29621905]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape=[3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the exact values for each element in the desired tensor by supplying a Python list (or list of lists) containing the numerical values. Here, the outermost list corresponds to axis 0, and the inner list to axis 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[2, 1, 4, 3],\n",
       "       [1, 2, 3, 4],\n",
       "       [4, 3, 2, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the unary scalar operators that take only one input. f:R->R.\n",
    "\n",
    "This just means that the function is mapping from any real number onto another.\n",
    "Binary scalar operators will take two inputs and will look like: f:R,R->R.\n",
    "\n",
    "For example, given two vectors of the same shape we can produce the third vector by lifting a scalar function to element-wise vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=\n",
       "array([2.7182817e+00, 7.3890562e+00, 5.4598148e+01, 2.9809580e+03],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1.0, 2, 4, 8])\n",
    "y = tf.constant([2.0, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation\n",
    "tf.exp(x) #exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate multiple tensors together by stacking them together end to end.\n",
    "We just need to give tensors to the system and define on which axis we want to stack them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(6, 4), dtype=float32, numpy=\n",
       " array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
       " array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4))\n",
    "Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "tf.concat([X, Y], axis=0), tf.concat([X, Y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical statement will be defined as usual. Here is the element-wise comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False,  True, False,  True],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X==Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing all elements of the tensor can give the tensor of just one element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=66.0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasing Mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have done operations on the tensors of thesame shape. However, it is not necessary requirement.\n",
    "\n",
    "Under *certain* conditions, even when shapes differ, we can still perform elementwise operations by invoking the broadcasting mechanism. This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the elementwise operations on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
       " array([[0],\n",
       "        [1],\n",
       "        [2]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.reshape(tf.range(3), (3, 1))\n",
    "b = tf.reshape(tf.range(2), (1, 2))\n",
    "a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any Python array, the first element has index 0 and ranges are specified to include the first but before the last element. As in standard Python lists, we can access elements according to their relative position to the end of the list by using negative indices.\n",
    "\n",
    "Thus, [-1] selects the last element and [1:3] selects the second and the third elements as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=float32)>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors in TensorFlow are immutable, and cannot be assigned to. Variables in TensorFlow are mutable containers of state that support assignments. Keep in mind that gradients in TensorFlow do not flow backwards through Variable assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  9.,  7.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_var = tf.Variable(X)\n",
    "X_var[1, 2].assign(9)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to assign multiple elements the same value, we simply index all of them and then assign them the value. For instance, [0:2, :] accesses the first and second rows, where : takes all the elements along axis 1 (column). While we discussed indexing for matrices, this obviously also works for vectors and for tensors of more than 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
       "array([[12., 12., 12., 12.],\n",
       "       [12., 12., 12., 12.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_var = tf.Variable(X)\n",
    "X_var[0:2, :].assign(tf.ones(X_var[0:2,:].shape, dtype = tf.float32) * 12)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Memory\n",
    "\n",
    "Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. In the following example, we demonstrate this with Python’s id() function, which gives us the exact address of the referenced object in memory. After running Y = Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then makes Y point to this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables are mutable containers of state in TensorFlow. They provide a way to store your model parameters. We can assign the result of an operation to a Variable with assign. To illustrate this concept, we create a Variable Z with the same shape as another tensor Y, using zeros_like to allocate a block of 0 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 140309024472800\n",
      "id(Z): 140309024472800\n"
     ]
    }
   ],
   "source": [
    "Z = tf.Variable(tf.zeros_like(Y))\n",
    "print('id(Z):', id(Z))\n",
    "Z.assign(X + Y)\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, TensorFlow provides the tf.function decorator to wrap computation inside of a TensorFlow graph that gets compiled and optimized before running. This allows TensorFlow to prune unused values, and to re-use prior allocations that are no longer needed. This minimizes the memory overhead of TensorFlow computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 8.,  9., 26., 27.],\n",
       "       [24., 33., 42., 51.],\n",
       "       [56., 57., 58., 59.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def computation(X, Y):\n",
    "    Z = tf.zeros_like(Y)  # This unused value will be pruned out\n",
    "    A = X + Y  # Allocations will be re-used when no longer needed\n",
    "    B = A + Y\n",
    "    C = B + Y\n",
    "    return C + Y\n",
    "\n",
    "computation(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to other Python Object\n",
    "\n",
    "Converting to a NumPy tensor (ndarray), or vice versa, is easy. The converted result does not share memory. This minor inconvenience is actually quite important: when you perform operations on the CPU or on GPUs, you do not want to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, tensorflow.python.framework.ops.EagerTensor)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = tf.constant(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a size-1 tensor to a Python scalar, we can invoke the item function or Python’s built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.5], dtype=float32), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([3.5]).numpy()\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Preprocessing\n",
    "### Reading the Dataset\n",
    "Important note: pandas can work with tensors.\n",
    "1. We write the dataset row by row into csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/sofyakonchakova/Desktop/Data Science/D2AI')\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "   f.write('NumRooms,Alley,Price\\n')  # Column names\n",
    "   f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
    "   f.write('2,NA,106000\\n')\n",
    "   f.write('4,NA,178100\\n')\n",
    "   f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "Typical methods for handling missing data: imputation and deletion.\n",
    "Imputation substitutes missing values by replacing missing values with substituted once.\n",
    "Deletion just ignores missing values.\n",
    "### Imputation\n",
    "In this example we split data in inputs and ouputs. For numerical values that are missing in input, we impute mean values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical or discrete values in inputs, we consider “NaN” as a category. Since the “Alley” column only takes two types of categorical values “Pave” and “NaN”, pandas can automatically convert this column to two columns “Alley_Pave” and “Alley_nan”. A row whose alley type is “Pave” will set values of “Alley_Pave” and “Alley_nan” to 1 and 0. A row with a missing alley type will set their values to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0           1          0\n",
      "1       2.0           0          1\n",
      "2       4.0           0          1\n",
      "3       3.0           0          1\n"
     ]
    }
   ],
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to Tensor Format\n",
    "Now that all the entries in inputs and outputs are numerical, they can be converted to the tensor format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 3), dtype=float64, numpy=\n",
       " array([[3., 1., 0.],\n",
       "        [2., 0., 1.],\n",
       "        [4., 0., 1.],\n",
       "        [3., 0., 1.]])>,\n",
       " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([127500, 106000, 178100, 140000])>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X, y = tf.constant(inputs.values), tf.constant(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "### Scalars\n",
    "We call values consisting of one numerical quantity - scalar.\n",
    "A scalar is represented by tensor with just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=5.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = tf.constant(3.0)\n",
    "y = tf.constant(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "Can be thought as simly list ofscalars.\n",
    "These values are called elements. We work with vectors via one-dimensional tensors. In general tensors can have arbitrary lengths, subject to the memory limits of your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(4)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to any element of a vector using a subscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length, Dimensionality and Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of a vector is commonly called the dimension of the vector. And just as any array has a length, so does a vector.\n",
    "In math notation it will look like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x \\in R^{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "Just as vectors generalize scalars from order zero to order one, matrices generalize vectors from order one to order two. To express matrix A, we can use math notation.\n",
    "$$\n",
    "A \\in R^{m*n}\n",
    "$$\n",
    "It means that matrix A contains m rows and n columns. Specifically, when a matrix has the same number of rows and columns, its shape becomes a square; thus, it is called a square matrix. We can create an m×n matrix by specifying a shape with two components m and n when calling any of our favorite functions for instantiating a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15],\n",
       "       [16, 17, 18, 19]], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.reshape(tf.range(20), (5, 4))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transpose matrix, use the function below. As a special type of the square matrix, a symmetric matrix A is equal to its transpose: \n",
    "$$\n",
    "A = A^{T}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 5), dtype=int32, numpy=\n",
       "array([[ 0,  4,  8, 12, 16],\n",
       "       [ 1,  5,  9, 13, 17],\n",
       "       [ 2,  6, 10, 14, 18],\n",
       "       [ 3,  7, 11, 15, 19]], dtype=int32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a symmetric matrix B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=bool, numpy=\n",
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = tf.constant([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B\n",
    "\n",
    "tf.transpose(B)==B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "Tensors (“tensors” in this subsection refer to algebraic objects) give us a generic way of describing n-dimensional arrays with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors. Tensors have similar to matrices indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=\n",
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]],\n",
       "\n",
       "       [[12, 13, 14, 15],\n",
       "        [16, 17, 18, 19],\n",
       "        [20, 21, 22, 23]]], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(tf.range(24), (2, 3, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Properties of Tensor Arithmetic\n",
    "Given any two tensors with the same shape, the result of any binary elementwise operation will be a tensor of that same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       " array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       " array([[ 0.,  2.,  4.,  6.],\n",
       "        [ 8., 10., 12., 14.],\n",
       "        [16., 18., 20., 22.],\n",
       "        [24., 26., 28., 30.],\n",
       "        [32., 34., 36., 38.]], dtype=float32)>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.reshape(tf.range(20, dtype=tf.float32), (5, 4))\n",
    "B = A  # No cloning of `A` to `B` by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction\n",
    "The useful function to perform on arbitrary tensors - reduction sum. This way, sum reduction leads to the sum of all elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(4, dtype=tf.float32)\n",
    "x, tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([5, 4]), <tf.Tensor: shape=(), dtype=float32, numpy=190.0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, tf.reduce_sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, invoking the function for calculating the sum reduces a tensor along all its axes to a scalar. We can also specify the axes along which the tensor is reduced via summation. Take matrices as an example.\n",
    "When we set axis = 0, we sum up elements of all rows with regard to column. Eg, sum of all row elements of the first column.\n",
    "\n",
    "When we set axis = 1, we get all columns sum row-wise. See example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]\n",
      " [12. 13. 14. 15.]\n",
      " [16. 17. 18. 19.]], shape=(5, 4), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([40., 45., 50., 55.], dtype=float32)>,\n",
       " TensorShape([4]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(A)\n",
    "A_sum_axis0 = tf.reduce_sum(A, axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 6., 22., 38., 54., 70.], dtype=float32)>,\n",
       " TensorShape([5]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = tf.reduce_sum(A, axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=190.0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(A, axis=[0, 1])  # Same as `tf.reduce_sum(A)`, reducing along rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=9.5>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.5>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(A), tf.reduce_sum(A) / tf.size(A).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(A, axis=0), tf.reduce_sum(A, axis=0) / A.shape[0] #reducing along the specified axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[0.        , 0.16666667, 0.33333334, 0.5       ],\n",
       "       [0.18181819, 0.22727273, 0.27272728, 0.3181818 ],\n",
       "       [0.21052632, 0.23684211, 0.2631579 , 0.28947368],\n",
       "       [0.22222222, 0.24074075, 0.25925925, 0.2777778 ],\n",
       "       [0.22857143, 0.24285714, 0.25714287, 0.27142859]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = tf.reduce_sum(A, axis=1, keepdims=True)\n",
    "sum_A\n",
    "\n",
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to calculate the cumulative sum of elements of A along some axis, say axis=0 (row by row), we can call the cumsum function. This function will not reduce the input tensor along any axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  6.,  8., 10.],\n",
       "       [12., 15., 18., 21.],\n",
       "       [24., 28., 32., 36.],\n",
       "       [40., 45., 50., 55.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cumsum(A, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Products\n",
    "The most fundamental operation is dot function. Given two vectors $$ x,y \\in R^{d} $$, their dot product $$ x^{T}y \\space or \\space <x,y>$$  is a sum over the products of the elements at the same position\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.ones(4, dtype=tf.float32)\n",
    "x, y, tf.tensordot(x, y, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expressing matrix-vector products in code with tensors, we use the matvec function. When we call tf.linalg.matvec(A, x) with a matrix A and a vector x, the matrix-vector product is performed. Note that the column dimension of A (its length along axis 1) must be the same as the dimension of x (its length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([5, 4]),\n",
       " TensorShape([4]),\n",
       " <tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 14.,  38.,  62.,  86., 110.], dtype=float32)>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, tf.linalg.matvec(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix - Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[ 6.,  6.,  6.],\n",
       "       [22., 22., 22.],\n",
       "       [38., 38., 38.],\n",
       "       [54., 54., 54.],\n",
       "       [70., 70., 70.]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = tf.ones((4, 3), tf.float32)\n",
    "tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms\n",
    "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big a vector is. Shows the distance of origin. The notion of size under consideration here concerns not dimensionality but rather the magnitude of the components.\n",
    "\n",
    "In linear algebra, a vector norm is a function f that maps a vector to a scalar, satisfying a handful of properties. \n",
    "\n",
    "Given any vector x, the first property says that if we scale all the elements of a vector by a constant factor α, its norm also scales by the absolute value of the same constant factor:\n",
    "\n",
    "$$\n",
    "f(\\alpha x)=\\left | \\alpha \\right |f(x)\n",
    "$$\n",
    "\n",
    "The second property is the familiar triangle inequality:\n",
    "$$\n",
    "f(x+y)\\leqslant f(x)+f(y)\n",
    "$$\n",
    "\n",
    "The third property is norm should be non-negative:\n",
    "\n",
    "$$\n",
    "f(x)\\geq 0\n",
    "$$\n",
    "\n",
    "The last  property - smallest norm is achieved only when vector consists of zeros.\n",
    "\n",
    "Euclidean distance is also a norm. It is specified as L_2 norm. It is calculated in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = tf.constant([3.0, -4.0]) #L_2 norm\n",
    "tf.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.abs(u)) #L_1 norm which is calculated as an absolute sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frobenius norm is similar to the L_2 norm but specified as square root of the sum of the squares of the matrix elements. Satisfies all the properties of vector norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm(tf.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Elementwise multiplication of two matrices is called their Hadamard product. It is different from matrix multiplication.\n",
    "In deep learning, we often work with norms such as the L1 norm, the L2 norm, and the Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.13 Exercises\n",
    "1.Prove that the transpose of a matrix A’s transpose is A: (A⊤)⊤=A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.reshape(tf.range(20, dtype=tf.float32), (5, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]])>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_t = tf.transpose(A)\n",
    "A_t_t = tf.transpose(A_t)\n",
    "A_t_t == A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Given two matrices A and B, show that the sum of transposes is equal to the transpose of a sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.reshape(tf.range(20, dtype=tf.float32), (5, 4))\n",
    "\n",
    "B = tf.reshape(tf.ones(20), (5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 5), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_transpose = tf.transpose(A+B)\n",
    "transp_sum = tf.transpose(A)+tf.transpose(B)\n",
    "sum_transpose == transp_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Given any square matrix A, is A+A⊤ always symmetric? Why?\n",
    "\n",
    "Because equal matrices have equal dimensions, only square matrices can be symmetric. and. Every square diagonal matrix is symmetric, since all off-diagonal elements are zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
       "array([[ True,  True],\n",
       "       [ True,  True]])>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.reshape(tf.range(4, dtype=tf.float32), (2, 2))\n",
    "C_trans = tf.transpose(C)\n",
    "D = C_trans + C\n",
    "D_transp = tf.transpose(D)\n",
    "D == D_transp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We defined the tensor X of shape (2, 3, 4) in this section. What is the output of len(X)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(tf.range(24), (2, 3, 4))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]\n",
      " [12. 13. 14. 15.]\n",
      " [16. 17. 18. 19.]], shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "\n",
    "#A/A.sum(axis=1)\n",
    "\n",
    "#tensorflow object has no attribute sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus\n",
    "### Derivatives and Differentiation\n",
    "\n",
    "Suppose we have a function that has input and output both scalars.\n",
    "$$\n",
    "f: R \\rightarrow R\n",
    "\n",
    "$$\n",
    "Then the derivative of x is defined as:\n",
    "\n",
    "$$\n",
    "f'(x)=\\lim_{h\\rightarrow 0} \\frac {f(x+h)-f(x)}{h}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the limit exists for the function f(a) for example, it is said to be differentiable at a. If f is differentiable at every number of an interval, then this function is differentiable on this interval. \n",
    "Let us experiment with an example.\n",
    "$$\n",
    "u = f(x) = 3x^{2}- 4x\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "import tensorflow as d2l\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "def numerical_lim(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "h = 0.1\n",
    "for i in range(5):\n",
    "    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
    "    h *= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further code is the visual interpretation of the derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_svg_display():  #@save\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_figsize(figsize=(3.5, 2.5)):  #@save\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.529359pt\" height=\"180.65625pt\" viewBox=\"0 0 243.529359 180.65625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-07-08T12:14:12.497750</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 243.529359 180.65625 \nL 243.529359 0 \nL 0 0 \nL 0 180.65625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 143.1 \nL 235.903125 143.1 \nL 235.903125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 49.480398 143.1 \nL 49.480398 7.2 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"mab9486bde7\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mab9486bde7\" x=\"49.480398\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(46.299148 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 110.702968 143.1 \nL 110.702968 7.2 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mab9486bde7\" x=\"110.702968\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(107.521718 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 171.925539 143.1 \nL 171.925539 7.2 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mab9486bde7\" x=\"171.925539\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(168.744289 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 233.148109 143.1 \nL 233.148109 7.2 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mab9486bde7\" x=\"233.148109\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(229.966859 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- x -->\n     <g transform=\"translate(135.29375 171.376563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 40.603125 114.635514 \nL 235.903125 114.635514 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m6f01aa27fd\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m6f01aa27fd\" x=\"40.603125\" y=\"114.635514\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 118.434732)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 40.603125 77.490157 \nL 235.903125 77.490157 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m6f01aa27fd\" x=\"40.603125\" y=\"77.490157\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 81.289376)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 40.603125 40.344801 \nL 235.903125 40.344801 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m6f01aa27fd\" x=\"40.603125\" y=\"40.344801\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 44.14402)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 83.771094)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 49.480398 114.635514 \nL 55.602655 117.38427 \nL 61.724912 119.687282 \nL 67.847169 121.54455 \nL 73.969426 122.956073 \nL 80.091683 123.921853 \nL 86.21394 124.441888 \nL 92.336197 124.516178 \nL 98.458454 124.144725 \nL 104.580711 123.327527 \nL 110.702968 122.064585 \nL 116.825225 120.355898 \nL 122.947482 118.201468 \nL 129.069739 115.601293 \nL 135.191996 112.555374 \nL 141.314254 109.06371 \nL 147.436511 105.126302 \nL 153.558768 100.74315 \nL 159.681025 95.914254 \nL 165.803282 90.639614 \nL 171.925539 84.919229 \nL 178.047796 78.7531 \nL 184.170053 72.141226 \nL 190.29231 65.083608 \nL 196.414567 57.580247 \nL 202.536824 49.63114 \nL 208.659081 41.23629 \nL 214.781338 32.395695 \nL 220.903595 23.109356 \nL 227.025852 13.377273 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 49.480398 136.922727 \nL 55.602655 135.436913 \nL 61.724912 133.951099 \nL 67.847169 132.465285 \nL 73.969426 130.97947 \nL 80.091683 129.493656 \nL 86.21394 128.007842 \nL 92.336197 126.522028 \nL 98.458454 125.036213 \nL 104.580711 123.550399 \nL 110.702968 122.064585 \nL 116.825225 120.578771 \nL 122.947482 119.092956 \nL 129.069739 117.607142 \nL 135.191996 116.121328 \nL 141.314254 114.635514 \nL 147.436511 113.149699 \nL 153.558768 111.663885 \nL 159.681025 110.178071 \nL 165.803282 108.692257 \nL 171.925539 107.206442 \nL 178.047796 105.720628 \nL 184.170053 104.234814 \nL 190.29231 102.749 \nL 196.414567 101.263185 \nL 202.536824 99.777371 \nL 208.659081 98.291557 \nL 214.781338 96.805743 \nL 220.903595 95.319928 \nL 227.025852 93.834114 \n\" clip-path=\"url(#p406459e479)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 143.1 \nL 40.603125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 235.903125 143.1 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 143.1 \nL 235.903125 143.1 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 172.153125 44.55625 \nQ 174.153125 44.55625 174.153125 42.55625 \nL 174.153125 14.2 \nQ 174.153125 12.2 172.153125 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 49.603125 20.298437 \nL 59.603125 20.298437 \nL 69.603125 20.298437 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- f(x) -->\n     <g transform=\"translate(77.603125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 49.603125 34.976562 \nL 59.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(77.603125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p406459e479\">\n   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"135.9\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot data points.\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else d2l.plt.gca()\n",
    "\n",
    "    # Return True if `X` (tensor or list) has 1 axis\n",
    "    def has_one_axis(X):\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "\n",
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.529359pt\" height=\"180.65625pt\" viewBox=\"0 0 243.529359 180.65625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-07-08T12:14:12.700872</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 243.529359 180.65625 \nL 243.529359 0 \nL 0 0 \nL 0 180.65625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 143.1 \nL 235.903125 143.1 \nL 235.903125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 49.480398 143.1 \nL 49.480398 7.2 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m1028a01055\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m1028a01055\" x=\"49.480398\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(46.299148 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 110.702968 143.1 \nL 110.702968 7.2 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m1028a01055\" x=\"110.702968\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(107.521718 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 171.925539 143.1 \nL 171.925539 7.2 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m1028a01055\" x=\"171.925539\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(168.744289 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 233.148109 143.1 \nL 233.148109 7.2 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m1028a01055\" x=\"233.148109\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(229.966859 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- x -->\n     <g transform=\"translate(135.29375 171.376563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 40.603125 114.635514 \nL 235.903125 114.635514 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m6b388e6f50\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m6b388e6f50\" x=\"40.603125\" y=\"114.635514\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 118.434732)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 40.603125 77.490157 \nL 235.903125 77.490157 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m6b388e6f50\" x=\"40.603125\" y=\"77.490157\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 81.289376)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 40.603125 40.344801 \nL 235.903125 40.344801 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m6b388e6f50\" x=\"40.603125\" y=\"40.344801\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 44.14402)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 83.771094)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 49.480398 114.635514 \nL 55.602655 117.38427 \nL 61.724912 119.687282 \nL 67.847169 121.54455 \nL 73.969426 122.956073 \nL 80.091683 123.921853 \nL 86.21394 124.441888 \nL 92.336197 124.516178 \nL 98.458454 124.144725 \nL 104.580711 123.327527 \nL 110.702968 122.064585 \nL 116.825225 120.355898 \nL 122.947482 118.201468 \nL 129.069739 115.601293 \nL 135.191996 112.555374 \nL 141.314254 109.06371 \nL 147.436511 105.126302 \nL 153.558768 100.74315 \nL 159.681025 95.914254 \nL 165.803282 90.639614 \nL 171.925539 84.919229 \nL 178.047796 78.7531 \nL 184.170053 72.141226 \nL 190.29231 65.083608 \nL 196.414567 57.580247 \nL 202.536824 49.63114 \nL 208.659081 41.23629 \nL 214.781338 32.395695 \nL 220.903595 23.109356 \nL 227.025852 13.377273 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 49.480398 136.922727 \nL 55.602655 135.436913 \nL 61.724912 133.951099 \nL 67.847169 132.465285 \nL 73.969426 130.97947 \nL 80.091683 129.493656 \nL 86.21394 128.007842 \nL 92.336197 126.522028 \nL 98.458454 125.036213 \nL 104.580711 123.550399 \nL 110.702968 122.064585 \nL 116.825225 120.578771 \nL 122.947482 119.092956 \nL 129.069739 117.607142 \nL 135.191996 116.121328 \nL 141.314254 114.635514 \nL 147.436511 113.149699 \nL 153.558768 111.663885 \nL 159.681025 110.178071 \nL 165.803282 108.692257 \nL 171.925539 107.206442 \nL 178.047796 105.720628 \nL 184.170053 104.234814 \nL 190.29231 102.749 \nL 196.414567 101.263185 \nL 202.536824 99.777371 \nL 208.659081 98.291557 \nL 214.781338 96.805743 \nL 220.903595 95.319928 \nL 227.025852 93.834114 \n\" clip-path=\"url(#p05eb7a4cbf)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 143.1 \nL 40.603125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 235.903125 143.1 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 143.1 \nL 235.903125 143.1 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 172.153125 44.55625 \nQ 174.153125 44.55625 174.153125 42.55625 \nL 174.153125 14.2 \nQ 174.153125 12.2 172.153125 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 49.603125 20.298437 \nL 59.603125 20.298437 \nL 69.603125 20.298437 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- f(x) -->\n     <g transform=\"translate(77.603125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 49.603125 34.976562 \nL 59.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(77.603125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p05eb7a4cbf\">\n   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"135.9\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives\n",
    "So far we have dealt with the differentiation of functions of just one variable. In deep learning, functions often depend on many variables. Thus, we need to extend the ideas of differentiation to these multivariate functions.\n",
    "Then we treat other variables as constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain the gradient vector of the function. \n",
    "Suppose, we have the input function:\n",
    "$$\n",
    "f: R^{n} \\rightarrow R\n",
    "$$\n",
    "where input is an n-dimensional vecor x\n",
    "$$\n",
    "x = [x_{1},x_{2},...,x_{n}]^{T}\n",
    "$$\n",
    "\n",
    "The gradient function f(x) with respect to x is a vector of n partial derivatives:\n",
    "\n",
    "$$\n",
    "\\nabla_{x}f(x)= \\left[ \\frac {\\sigma f(x)}{\\sigma x_{1}}, \\frac {\\sigma f(x)}{\\sigma x_{2}},...,\\frac {\\sigma f(x)}{\\sigma x_{n}} \\right]^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "Gradients are hard to find as deep learing are often composite, so we may not apply the rules to differentiate these functions. Fortunately, chain rule is useful for differentiating such functions.\n",
    "\n",
    "$$\n",
    "\n",
    "\\frac {d_{y}}{d_{x}}=\\frac {d_{y}}{d_{u}}   \\frac {d_{u}}{d_{x}}\n",
    "\n",
    "$$\n",
    "\n",
    "where y = f(u) and u = g(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Automatic Differentiation\n",
    "Deep learning frameworks expedite this work by automatically calculating derivatives, i.e., automatic differentiation. In practice, based on our designed model the system builds a computational graph, tracking which data cmobined through which operations to produce the output. Automatic differentiation enables the system to subsequently backpropagate gradients. Here, backpropagate simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter.\n",
    "\n",
    "\n",
    "Toy example, we are interested in differentiating the function\n",
    "\n",
    "$$\n",
    "y = 2x^{T}x\n",
    "\n",
    "$$\n",
    "with respect to a column vector x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.range(4, dtype = tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(x) #not to allocate additional memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record all computations onto a tape\n",
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x is a vector of 4, an dot product of x and x is performed, yielding the scalar output that we assign to y. Next, we can automatically calculate the gradient of y with respect to each component of x by calling the function and printing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad = t.gradient(y, x)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the function we named before should be 4x. Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = tf.reduce_sum(x)\n",
    "t.gradient(y, x)  # Overwritten by the newly calculated gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward for Non-Scalar Variables\n",
    "When y is not a scalar, the most natural interpretation of the differentiation of a vector y with respect to a vector x is a matrix.\n",
    "For higher order and higher-dimensional x and y, it can result in a high-order tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = x * x\n",
    "t.gradient(y, x)  # Same as `y = tf.reduce_sum(x * x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detaching Computation\n",
    "\n",
    "Sometimes we wish to have some calculations outside of the recorded computational graph.\n",
    "E.g. say that y was calculated as a function of x, and z was subsequently calculated as a function of both x and y.\n",
    "Now imagine we want to calculate the gradient of z with respect to x, but want to treat y only as a constant.\n",
    "Here, we can detach y to return a new variable u that has the same value as y but discards any information on how it was computed in the computational graph. It wull not flow backwards through u to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set `persistent=True` to run `t.gradient` more than once\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x * x\n",
    "    u = tf.stop_gradient(y)\n",
    "    z = u * x\n",
    "\n",
    "x_grad = t.gradient(z, x)\n",
    "x_grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoking backpropagation as we know how the y was created\n",
    "t.gradient(y, x) == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Gradient of Python Control Flow\n",
    "The advantage of using automatic differentiation, we can invoke the gradient function even if the function itself was built with many different python features and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=409600.0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Variable(tf.random.normal(shape=()))\n",
    "with tf.GradientTape() as t:\n",
    "    d = f(a)\n",
    "d_grad = t.gradient(d, a)\n",
    "d_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_grad == d/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "### Basic Probability Theory\n",
    "One natural approach for each value is to take the individual count for that value and to divide it by the total number of tosses. This gives us an estimate of the probability of a given event. The law of large numbers tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from d2l import tensorflow as d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are casting a die.\n",
    "The process of drawing examples from propbability distribution - *sampling*.\n",
    "The distribution that assigns probabilities to a number of discrete choices is called the multinomial distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 0., 1., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fair_probs = tf.ones(6) / 6\n",
    "tfp.distributions.Multinomial(1, fair_probs).sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For drawing multiple samples at once:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 1., 0., 3., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfp.distributions.Multinomial(10, fair_probs).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to sample rolls of a die, we can simulate 1000 rolls. We can then go through and count, after each of the 1000 rolls, how many times each number was rolled. Specifically, we calculate the relative frequency as the estimate of the true probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0.183, 0.146, 0.165, 0.158, 0.176, 0.172], dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = tfp.distributions.Multinomial(1000, fair_probs).sample()\n",
    "counts / 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise how these probabilities converge over time towards true probabilities, run the code below.\n",
    "Note, black line gives the true underlying probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"392.14375pt\" height=\"289.37625pt\" viewBox=\"0 0 392.14375 289.37625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-07-08T12:24:54.620089</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 289.37625 \nL 392.14375 289.37625 \nL 392.14375 0 \nL 0 0 \nL 0 289.37625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 251.82 \nL 384.94375 251.82 \nL 384.94375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m11e2063134\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m11e2063134\" x=\"65.361932\" y=\"251.82\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.180682 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m11e2063134\" x=\"126.356649\" y=\"251.82\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(116.812899 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m11e2063134\" x=\"187.351365\" y=\"251.82\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(177.807615 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m11e2063134\" x=\"248.346082\" y=\"251.82\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(238.802332 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m11e2063134\" x=\"309.340799\" y=\"251.82\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(299.797049 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m11e2063134\" x=\"370.335515\" y=\"251.82\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(360.791765 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Groups of experiments -->\n     <g transform=\"translate(160.397656 280.096562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-47\" d=\"M 3809 666 \nL 3809 1919 \nL 2778 1919 \nL 2778 2438 \nL 4434 2438 \nL 4434 434 \nQ 4069 175 3628 42 \nQ 3188 -91 2688 -91 \nQ 1594 -91 976 548 \nQ 359 1188 359 2328 \nQ 359 3472 976 4111 \nQ 1594 4750 2688 4750 \nQ 3144 4750 3555 4637 \nQ 3966 4525 4313 4306 \nL 4313 3634 \nQ 3963 3931 3569 4081 \nQ 3175 4231 2741 4231 \nQ 1884 4231 1454 3753 \nQ 1025 3275 1025 2328 \nQ 1025 1384 1454 906 \nQ 1884 428 2741 428 \nQ 3075 428 3337 486 \nQ 3600 544 3809 666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-47\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"77.490234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"116.353516\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"177.535156\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"240.914062\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"304.390625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"356.490234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"388.277344\"/>\n      <use xlink:href=\"#DejaVuSans-66\" x=\"449.458984\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"484.664062\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"516.451172\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"576.224609\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"635.404297\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"698.880859\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"760.404297\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"801.517578\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"829.300781\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"926.712891\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"988.236328\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"1051.615234\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"1090.824219\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m8e8c6f4fa4\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m8e8c6f4fa4\" x=\"50.14375\" y=\"215.991822\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 219.791041)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m8e8c6f4fa4\" x=\"50.14375\" y=\"166.573642\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 170.372861)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m8e8c6f4fa4\" x=\"50.14375\" y=\"117.155462\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.20 -->\n      <g transform=\"translate(20.878125 120.954681)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m8e8c6f4fa4\" x=\"50.14375\" y=\"67.737283\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.25 -->\n      <g transform=\"translate(20.878125 71.536501)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m8e8c6f4fa4\" x=\"50.14375\" y=\"18.319103\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.30 -->\n      <g transform=\"translate(20.878125 22.118321)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- Estimated probability -->\n     <g transform=\"translate(14.798438 183.033437)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"63.183594\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"115.283203\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"154.492188\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"182.275391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"279.6875\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"340.966797\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"380.175781\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"441.699219\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"505.175781\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"536.962891\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"600.439453\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"639.302734\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"700.484375\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"763.960938\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"825.240234\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"888.716797\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"916.5\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"944.283203\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"972.066406\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"1011.275391\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 65.361932 215.991821 \nL 65.971879 117.155459 \nL 66.581826 117.155459 \nL 67.191773 92.446378 \nL 67.80172 117.155459 \nL 68.411668 133.628185 \nL 69.021615 159.513897 \nL 69.631562 178.928184 \nL 70.241509 172.064545 \nL 71.461403 180.051324 \nL 72.681298 185.580627 \nL 73.901192 189.635461 \nL 74.511139 191.282732 \nL 75.731034 205.010004 \nL 76.340981 200.386082 \nL 77.560875 201.872342 \nL 78.170822 207.006698 \nL 78.780769 207.39736 \nL 79.390717 187.164554 \nL 80.610611 189.382026 \nL 81.220558 183.046362 \nL 81.830505 184.222993 \nL 82.440452 178.502169 \nL 83.0504 183.046362 \nL 83.660347 184.109132 \nL 84.880241 180.051324 \nL 86.710083 183.046362 \nL 87.32003 181.26554 \nL 87.929977 182.179383 \nL 89.149871 178.928184 \nL 89.759819 182.242815 \nL 90.369766 183.046362 \nL 90.979713 179.215508 \nL 91.58966 177.805045 \nL 92.199607 178.653644 \nL 92.809554 177.316731 \nL 94.639396 185.735799 \nL 95.249343 184.364188 \nL 95.85929 186.922297 \nL 96.469237 187.481334 \nL 97.079185 186.154436 \nL 97.689132 188.537271 \nL 98.909026 189.517801 \nL 99.518973 183.046362 \nL 100.738868 184.163168 \nL 101.958762 181.966184 \nL 103.178656 186.184024 \nL 105.618445 187.963595 \nL 106.838339 185.911192 \nL 107.448286 179.281179 \nL 108.058234 179.798227 \nL 108.668181 177.555453 \nL 109.278128 178.081986 \nL 110.498022 173.821641 \nL 111.107969 174.37651 \nL 111.717917 173.633376 \nL 112.327864 171.642167 \nL 112.937811 172.20356 \nL 113.547758 173.986371 \nL 114.157705 172.064545 \nL 114.767652 173.805573 \nL 115.987547 172.45676 \nL 117.817388 177.366113 \nL 119.037283 176.013074 \nL 119.64723 176.457278 \nL 122.087018 173.933793 \nL 122.696966 175.416894 \nL 123.306913 173.780462 \nL 123.91686 174.215623 \nL 124.526807 173.633376 \nL 125.136754 172.064545 \nL 125.746701 171.515464 \nL 126.356649 171.955825 \nL 126.966596 173.356523 \nL 128.18649 174.176433 \nL 128.796437 175.515982 \nL 130.016332 176.272533 \nL 131.236226 175.187818 \nL 132.45612 175.923028 \nL 133.066067 175.398322 \nL 133.676015 175.757547 \nL 134.285962 173.50953 \nL 135.505856 170.833834 \nL 136.72575 169.924026 \nL 137.335698 171.141703 \nL 137.945645 170.691829 \nL 138.555592 171.066209 \nL 139.165539 169.814172 \nL 139.775486 169.386065 \nL 140.385433 168.167784 \nL 140.995381 169.341057 \nL 142.215275 168.519233 \nL 144.045116 169.614758 \nL 144.655064 170.723258 \nL 145.874958 171.40399 \nL 146.484905 172.474316 \nL 147.094852 172.796677 \nL 147.704799 172.38754 \nL 148.314747 171.262957 \nL 148.924694 171.587085 \nL 150.754535 170.428968 \nL 151.364482 167.965705 \nL 151.97443 167.610383 \nL 152.584377 166.573636 \nL 153.194324 166.914452 \nL 153.804271 165.896675 \nL 154.414218 164.22039 \nL 155.024165 163.902388 \nL 155.634113 164.251981 \nL 156.24406 165.255825 \nL 156.854007 164.937279 \nL 157.463954 163.322453 \nL 158.073901 163.666685 \nL 158.683848 164.648261 \nL 159.293796 163.704196 \nL 159.903743 164.672945 \nL 160.51369 163.740751 \nL 161.123637 163.445916 \nL 161.733584 163.776392 \nL 162.343531 164.720456 \nL 162.953479 164.425032 \nL 163.563426 165.353441 \nL 164.173373 165.664108 \nL 165.393267 163.878102 \nL 166.003214 164.787438 \nL 166.613162 163.31855 \nL 167.223109 163.632089 \nL 167.833056 164.526742 \nL 169.05295 163.972684 \nL 169.662897 162.551233 \nL 170.272845 162.860133 \nL 171.492739 164.59692 \nL 173.32258 163.797335 \nL 173.932528 162.432454 \nL 174.542475 162.729999 \nL 175.152422 162.478213 \nL 175.762369 161.143067 \nL 176.372316 161.442792 \nL 177.592211 159.895508 \nL 178.202158 160.197103 \nL 179.422052 161.842121 \nL 180.031999 162.12862 \nL 180.641946 161.371731 \nL 181.251894 161.6577 \nL 181.861841 160.911149 \nL 182.471788 160.68443 \nL 183.081735 159.95059 \nL 183.691682 160.744829 \nL 184.301629 161.026703 \nL 184.911577 160.804005 \nL 185.521524 161.082728 \nL 186.131471 159.868659 \nL 188.57126 159.027026 \nL 189.181207 159.790749 \nL 189.791154 159.582779 \nL 190.401101 159.856612 \nL 191.011048 159.172846 \nL 191.620995 158.97084 \nL 192.84089 159.513897 \nL 193.450837 159.313157 \nL 194.060784 159.58051 \nL 194.670731 159.381332 \nL 195.280678 159.645858 \nL 195.890626 159.448226 \nL 198.940361 160.733312 \nL 200.160256 162.121551 \nL 201.38015 162.602545 \nL 201.990097 163.279094 \nL 202.600044 162.200359 \nL 203.819939 162.672207 \nL 204.429886 162.473441 \nL 206.259727 163.165499 \nL 207.479622 161.927483 \nL 208.089569 161.736965 \nL 208.699516 161.129268 \nL 209.309463 160.943727 \nL 209.91941 161.175012 \nL 210.529358 160.990841 \nL 211.139305 161.220005 \nL 211.749252 161.03716 \nL 212.359199 160.039001 \nL 212.969146 159.455796 \nL 213.579093 160.092565 \nL 214.798988 160.547035 \nL 215.408935 160.371362 \nL 216.628829 160.818114 \nL 217.238776 160.24812 \nL 217.848724 160.4702 \nL 218.458671 161.082728 \nL 220.898459 161.940694 \nL 221.508407 162.535578 \nL 222.728301 162.948367 \nL 223.338248 162.772238 \nL 223.948195 161.840103 \nL 224.558142 162.046793 \nL 226.997931 159.885464 \nL 227.607878 158.985082 \nL 228.217825 158.460214 \nL 228.827773 158.674075 \nL 229.43772 158.154244 \nL 230.657614 157.852782 \nL 231.267561 158.427783 \nL 231.877508 158.277162 \nL 233.097403 158.695387 \nL 233.70735 159.259033 \nL 234.317297 158.396531 \nL 234.927244 158.957217 \nL 235.537191 158.807921 \nL 236.147139 159.01143 \nL 236.757086 159.563971 \nL 237.367033 159.763356 \nL 237.97698 159.265322 \nL 239.806822 159.858276 \nL 240.416769 160.39637 \nL 241.026716 159.904757 \nL 242.856557 160.480996 \nL 243.466505 160.333114 \nL 244.076452 160.858615 \nL 244.686399 160.710469 \nL 246.51624 161.266987 \nL 248.346082 160.827348 \nL 248.956029 161.010002 \nL 249.565976 160.865272 \nL 250.175923 161.371731 \nL 250.785871 160.578655 \nL 251.395818 160.759733 \nL 252.005765 159.973815 \nL 253.225659 160.336398 \nL 253.835606 160.197103 \nL 254.445554 159.4231 \nL 255.055501 158.97084 \nL 255.665448 159.153022 \nL 256.275395 159.648803 \nL 256.885342 159.827672 \nL 257.495289 159.379859 \nL 258.715184 159.114288 \nL 259.325131 159.292597 \nL 259.935078 159.160916 \nL 261.154972 159.513897 \nL 261.76492 159.994758 \nL 262.374867 159.862532 \nL 262.984814 160.03523 \nL 263.594761 159.903697 \nL 264.204708 160.075231 \nL 264.814655 160.547035 \nL 265.424603 160.114731 \nL 266.03455 160.284056 \nL 267.864391 161.676345 \nL 268.474338 161.838969 \nL 269.084286 161.115512 \nL 269.694233 160.690527 \nL 270.30418 161.147913 \nL 271.524074 160.888365 \nL 272.743969 160.052182 \nL 273.963863 160.378358 \nL 274.57381 159.390773 \nL 275.183757 158.981871 \nL 275.793704 159.146631 \nL 276.403652 159.025627 \nL 277.013599 159.189312 \nL 278.233493 160.078677 \nL 278.84344 159.674812 \nL 279.453387 158.711661 \nL 280.673282 158.476885 \nL 281.283229 158.082063 \nL 281.893176 158.244732 \nL 282.503123 158.129634 \nL 283.723018 158.451995 \nL 284.332965 158.886376 \nL 284.942912 158.496989 \nL 285.552859 158.928836 \nL 287.992648 159.552484 \nL 288.602595 159.167647 \nL 289.212542 159.590643 \nL 290.432436 159.361258 \nL 292.262278 159.816729 \nL 292.872225 159.438402 \nL 293.482172 159.852768 \nL 294.092119 159.739217 \nL 295.312014 158.990958 \nL 296.531908 159.290963 \nL 297.751802 159.070355 \nL 298.36175 158.444808 \nL 298.971697 158.337281 \nL 299.581644 158.487033 \nL 300.191591 158.892061 \nL 300.801538 158.528815 \nL 301.411485 158.422186 \nL 302.021433 158.570186 \nL 302.63138 158.463999 \nL 303.241327 158.611129 \nL 303.851274 158.505369 \nL 304.461221 158.148662 \nL 305.071168 158.044611 \nL 306.291063 158.337281 \nL 308.120904 157.284511 \nL 308.730851 157.678375 \nL 309.340799 157.823783 \nL 309.950746 157.722618 \nL 310.560693 158.112462 \nL 311.17064 157.766447 \nL 313.000482 158.195615 \nL 313.610429 157.852782 \nL 314.220376 157.994934 \nL 314.830323 157.89533 \nL 316.660165 159.035274 \nL 317.270112 159.172846 \nL 317.880059 159.071607 \nL 319.099953 159.344601 \nL 320.929795 160.455207 \nL 322.149689 160.249991 \nL 322.759636 159.680792 \nL 323.369583 159.347399 \nL 323.979531 159.24812 \nL 324.589478 159.381332 \nL 325.809372 158.722162 \nL 326.419319 158.164495 \nL 327.029266 158.069118 \nL 327.639214 158.203509 \nL 328.859108 158.013918 \nL 329.469055 158.375234 \nL 330.079002 158.053271 \nL 330.688949 157.95947 \nL 331.298897 157.639921 \nL 331.908844 157.773149 \nL 332.518791 157.680614 \nL 333.128738 157.813149 \nL 333.738685 157.496841 \nL 334.348632 157.405558 \nL 334.95858 157.760925 \nL 335.568527 157.892075 \nL 336.178474 157.800528 \nL 336.788421 157.931001 \nL 337.398368 157.839777 \nL 338.008315 157.528344 \nL 341.058051 158.173641 \nL 342.277946 158.862237 \nL 343.49784 158.247162 \nL 344.107787 158.373246 \nL 344.717734 158.714106 \nL 347.157523 158.355072 \nL 347.76747 157.840263 \nL 348.377417 157.540214 \nL 348.987365 157.877745 \nL 349.597312 158.00218 \nL 350.817206 157.827995 \nL 351.427153 157.320874 \nL 352.647048 157.15068 \nL 353.256995 156.85717 \nL 354.476889 156.69001 \nL 355.086836 156.814592 \nL 355.696783 157.145849 \nL 358.136572 157.635223 \nL 358.746519 157.346176 \nL 359.356466 157.262964 \nL 361.186308 157.625488 \nL 361.796255 157.339445 \nL 362.406202 157.459639 \nL 363.016149 157.175098 \nL 363.626097 157.093418 \nL 364.236044 156.408194 \nL 364.845991 156.328414 \nL 365.455938 156.449432 \nL 366.065885 156.16981 \nL 366.675832 156.091002 \nL 367.28578 155.813236 \nL 369.725568 156.294658 \nL 369.725568 156.294658 \n\" clip-path=\"url(#p98521e957c)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 65.361932 215.991821 \nL 65.971879 215.991821 \nL 66.581826 183.046362 \nL 67.191773 141.864555 \nL 67.80172 117.155459 \nL 68.411668 117.155459 \nL 69.021615 145.394418 \nL 69.631562 154.219103 \nL 70.241509 172.064545 \nL 70.851456 176.457278 \nL 71.461403 162.081079 \nL 72.071351 174.810007 \nL 72.681298 177.977831 \nL 73.291245 173.633376 \nL 74.511139 178.928184 \nL 75.121086 169.480588 \nL 75.731034 166.573636 \nL 76.340981 169.174604 \nL 76.950928 176.457278 \nL 77.560875 178.339869 \nL 78.170822 171.066209 \nL 78.780769 155.830556 \nL 80.000664 160.643457 \nL 80.610611 166.573636 \nL 81.220558 164.743343 \nL 81.830505 159.513897 \nL 83.660347 155.414703 \nL 85.490188 152.038878 \nL 86.100135 153.866108 \nL 86.710083 141.864555 \nL 87.32003 138.525491 \nL 87.929977 140.564079 \nL 88.539924 139.963849 \nL 89.149871 144.335462 \nL 89.759819 134.029959 \nL 90.979713 133.245101 \nL 91.58966 128.386868 \nL 92.809554 127.898539 \nL 94.029449 131.569096 \nL 94.639396 129.257868 \nL 95.249343 129.015832 \nL 96.469237 124.758256 \nL 97.079185 126.479652 \nL 97.689132 126.306984 \nL 98.909026 129.510007 \nL 99.518973 132.761206 \nL 100.12892 132.492144 \nL 100.738868 133.907394 \nL 101.958762 139.839222 \nL 102.568709 141.067489 \nL 103.178656 137.550263 \nL 103.788603 138.775922 \nL 105.008498 132.130666 \nL 105.618445 133.382335 \nL 106.228392 136.050652 \nL 107.448286 138.334678 \nL 108.058234 136.644328 \nL 108.668181 136.373647 \nL 109.278128 137.464297 \nL 110.498022 136.922727 \nL 111.107969 137.963111 \nL 111.717917 133.842121 \nL 112.327864 134.895318 \nL 112.937811 132.168575 \nL 113.547758 133.216367 \nL 114.157705 131.797892 \nL 114.767652 134.029959 \nL 115.3776 133.826656 \nL 117.207441 129.797317 \nL 117.817388 131.924109 \nL 118.427335 132.879426 \nL 119.64723 132.530009 \nL 120.257177 130.188825 \nL 120.867124 130.047158 \nL 121.477071 132.034052 \nL 122.087018 129.772869 \nL 122.696966 129.640054 \nL 123.306913 131.569096 \nL 123.91686 130.401567 \nL 125.136754 130.133979 \nL 125.746701 130.992548 \nL 126.356649 130.855551 \nL 126.966596 131.690217 \nL 127.576543 129.62995 \nL 128.18649 129.510007 \nL 128.796437 128.451052 \nL 129.406384 128.344481 \nL 130.016332 129.163625 \nL 131.236226 128.943283 \nL 132.45612 132.292568 \nL 133.066067 130.39248 \nL 135.505856 129.936023 \nL 136.115803 130.671542 \nL 137.335698 127.122151 \nL 137.945645 127.039101 \nL 139.165539 128.497341 \nL 139.775486 127.601584 \nL 140.995381 130.597211 \nL 141.605328 131.274939 \nL 142.215275 132.720248 \nL 142.825222 132.598641 \nL 143.435169 130.946583 \nL 144.045116 132.361052 \nL 144.655064 129.981561 \nL 145.265011 130.633147 \nL 145.874958 130.531805 \nL 146.484905 131.907157 \nL 147.094852 131.797892 \nL 147.704799 132.416959 \nL 148.924694 132.195777 \nL 149.534641 133.509685 \nL 151.364482 135.25226 \nL 152.584377 135.000916 \nL 153.194324 135.559481 \nL 153.804271 136.787335 \nL 154.414218 136.653798 \nL 155.024165 137.857675 \nL 155.634113 138.382057 \nL 156.24406 138.240553 \nL 156.854007 137.446373 \nL 157.463954 138.613357 \nL 159.293796 140.111009 \nL 159.903743 138.696716 \nL 160.51369 139.189051 \nL 161.123637 138.424046 \nL 161.733584 137.047059 \nL 162.343531 136.922727 \nL 162.953479 137.413839 \nL 163.563426 137.288801 \nL 164.78332 139.453915 \nL 165.393267 139.91778 \nL 166.003214 140.971449 \nL 166.613162 141.420675 \nL 167.833056 139.963849 \nL 169.05295 142.009049 \nL 169.662897 141.864555 \nL 170.882792 142.716586 \nL 171.492739 142.00575 \nL 172.102686 142.426125 \nL 172.712633 140.049756 \nL 173.32258 139.365887 \nL 174.542475 140.217284 \nL 175.152422 140.08986 \nL 175.762369 141.049963 \nL 176.372316 140.919401 \nL 176.982263 141.327404 \nL 177.592211 142.265239 \nL 178.812105 141.996693 \nL 179.422052 142.390278 \nL 180.031999 142.256756 \nL 180.641946 143.165032 \nL 181.251894 142.511384 \nL 181.861841 142.37932 \nL 182.471788 143.272839 \nL 183.081735 143.647675 \nL 183.691682 144.525538 \nL 184.301629 144.38589 \nL 185.521524 146.107521 \nL 186.131471 145.96204 \nL 186.741418 146.312193 \nL 187.351365 146.167125 \nL 187.961312 147.002086 \nL 189.181207 146.709475 \nL 189.791154 145.601048 \nL 190.401101 145.942747 \nL 191.011048 146.758621 \nL 191.620995 147.091469 \nL 192.230943 146.948241 \nL 192.84089 146.335728 \nL 194.060784 146.992852 \nL 194.670731 146.388748 \nL 196.500573 147.355464 \nL 197.11052 148.127228 \nL 198.330414 147.844381 \nL 199.550309 148.461092 \nL 200.160256 147.874868 \nL 200.770203 148.623539 \nL 201.38015 148.924295 \nL 201.990097 148.343829 \nL 202.600044 148.20583 \nL 203.209992 148.939847 \nL 205.64978 148.389471 \nL 206.259727 146.976784 \nL 207.479622 146.72189 \nL 208.089569 146.175505 \nL 208.699516 146.471327 \nL 211.139305 142.688191 \nL 211.749252 142.992349 \nL 212.359199 143.702418 \nL 214.189041 143.377347 \nL 214.798988 143.672536 \nL 215.408935 143.565185 \nL 216.018882 142.661622 \nL 216.628829 142.956118 \nL 217.238776 143.64361 \nL 217.848724 143.538071 \nL 218.458671 143.041171 \nL 219.068618 143.329511 \nL 220.288512 143.124236 \nL 221.508407 142.152984 \nL 222.118354 142.05609 \nL 222.728301 142.341558 \nL 223.338248 141.864555 \nL 223.948195 142.527246 \nL 224.558142 142.807648 \nL 225.16809 142.710106 \nL 225.778037 143.362074 \nL 226.387984 143.263178 \nL 226.997931 143.908165 \nL 227.607878 144.17814 \nL 228.217825 144.814895 \nL 228.827773 144.344652 \nL 230.047667 144.873408 \nL 230.657614 145.498248 \nL 231.877508 146.012792 \nL 232.487456 145.907857 \nL 234.317297 146.664157 \nL 234.927244 146.204135 \nL 235.537191 145.394418 \nL 237.97698 144.996694 \nL 238.586927 145.592594 \nL 239.196874 145.83874 \nL 239.806822 144.705674 \nL 241.026716 145.198995 \nL 242.24661 145.006253 \nL 242.856557 145.249364 \nL 244.076452 145.058242 \nL 244.686399 145.298702 \nL 245.296346 145.203619 \nL 245.906293 145.441959 \nL 246.51624 145.347038 \nL 247.126188 145.583316 \nL 247.736135 146.147463 \nL 248.346082 145.066062 \nL 248.956029 145.300911 \nL 249.565976 145.208023 \nL 250.175923 145.440869 \nL 250.785871 145.996238 \nL 251.395818 145.578986 \nL 252.005765 146.13029 \nL 252.615712 145.715321 \nL 253.225659 145.62289 \nL 253.835606 145.849889 \nL 254.445554 145.757619 \nL 255.055501 146.299513 \nL 256.275395 146.743422 \nL 256.885342 146.021953 \nL 257.495289 145.930611 \nL 258.715184 146.371237 \nL 259.325131 146.279659 \nL 259.935078 146.497513 \nL 260.545025 147.021895 \nL 261.154972 146.315256 \nL 262.374867 146.135254 \nL 262.984814 146.654319 \nL 263.594761 146.563832 \nL 264.204708 146.776147 \nL 265.424603 146.596086 \nL 266.03455 146.806369 \nL 266.644497 145.820993 \nL 267.254444 146.330043 \nL 267.864391 145.945634 \nL 269.084286 146.953882 \nL 269.694233 146.571048 \nL 270.914127 146.98182 \nL 271.524074 146.602286 \nL 272.134021 147.097065 \nL 272.743969 147.299101 \nL 273.963863 147.12334 \nL 275.183757 147.52258 \nL 275.793704 147.434802 \nL 276.403652 147.91721 \nL 278.84344 148.692995 \nL 280.063335 148.514303 \nL 280.673282 148.704925 \nL 281.283229 149.17287 \nL 281.893176 149.082942 \nL 282.503123 149.270353 \nL 283.11307 148.628488 \nL 283.723018 149.09144 \nL 284.332965 148.45364 \nL 284.942912 148.093163 \nL 285.552859 148.553759 \nL 286.162806 148.467263 \nL 286.772753 147.838181 \nL 287.382701 147.754115 \nL 287.992648 147.130424 \nL 289.212542 147.504671 \nL 289.822489 147.422416 \nL 291.042384 147.792068 \nL 291.652331 147.178333 \nL 292.262278 147.097846 \nL 292.872225 147.546321 \nL 293.482172 147.465274 \nL 294.702067 147.828814 \nL 295.921961 147.666956 \nL 296.531908 147.846753 \nL 297.141855 147.766192 \nL 297.751802 147.427335 \nL 298.36175 147.348291 \nL 299.581644 148.218319 \nL 300.191591 147.369691 \nL 300.801538 147.291619 \nL 301.411485 147.723408 \nL 302.021433 147.644835 \nL 303.241327 148.499988 \nL 303.851274 148.672155 \nL 304.461221 148.591963 \nL 305.681116 149.433669 \nL 306.291063 149.601743 \nL 307.510957 150.43202 \nL 308.120904 150.100911 \nL 309.340799 148.950702 \nL 310.560693 149.283417 \nL 312.390534 149.046005 \nL 313.000482 149.210499 \nL 313.610429 149.616426 \nL 314.830323 149.458073 \nL 316.050217 149.781053 \nL 316.660165 149.702053 \nL 317.880059 150.497853 \nL 318.490006 150.655293 \nL 319.099953 149.626883 \nL 319.7099 149.31274 \nL 320.319848 149.471888 \nL 320.929795 149.394935 \nL 321.539742 148.84883 \nL 322.759636 148.231663 \nL 323.369583 148.391488 \nL 323.979531 148.317982 \nL 324.589478 148.708857 \nL 325.199425 148.634953 \nL 325.809372 149.023265 \nL 327.029266 149.334743 \nL 328.249161 149.185757 \nL 328.859108 148.883528 \nL 329.469055 149.038155 \nL 330.079002 148.96487 \nL 330.688949 149.345288 \nL 331.298897 149.27162 \nL 331.908844 148.972646 \nL 333.128738 149.277275 \nL 333.738685 148.756207 \nL 334.348632 148.908315 \nL 334.95858 148.83665 \nL 335.568527 148.987889 \nL 336.178474 148.916357 \nL 336.788421 149.288365 \nL 338.008315 149.144902 \nL 339.838157 149.589563 \nL 340.448104 149.299146 \nL 341.667998 149.59295 \nL 342.277946 149.956107 \nL 342.887893 149.667428 \nL 343.49784 150.028818 \nL 344.107787 149.95705 \nL 344.717734 149.670256 \nL 345.327681 149.169851 \nL 347.157523 149.602818 \nL 347.76747 149.53289 \nL 348.377417 149.675809 \nL 348.987365 149.393934 \nL 349.597312 149.536543 \nL 350.207259 149.889729 \nL 350.817206 150.030674 \nL 351.427153 149.960717 \nL 352.0371 150.100911 \nL 352.647048 149.821716 \nL 353.256995 149.752658 \nL 353.866942 149.892395 \nL 354.476889 149.823484 \nL 355.086836 150.170131 \nL 355.696783 150.308116 \nL 356.306731 150.652304 \nL 356.916678 150.788712 \nL 357.526625 151.130455 \nL 358.136572 150.648872 \nL 359.966414 150.441255 \nL 361.186308 150.711023 \nL 361.796255 150.642112 \nL 362.406202 150.776031 \nL 364.236044 151.778388 \nL 365.455938 152.038878 \nL 366.065885 152.368412 \nL 366.675832 152.496941 \nL 367.28578 152.824237 \nL 367.895727 152.951322 \nL 368.505674 153.276379 \nL 369.115621 153.402066 \nL 369.725568 153.724913 \nL 369.725568 153.724913 \n\" clip-path=\"url(#p98521e957c)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 65.361932 117.155459 \nL 65.971879 117.155459 \nL 66.581826 84.210008 \nL 67.191773 67.737283 \nL 67.80172 57.853656 \nL 68.411668 51.264542 \nL 69.021615 46.558049 \nL 69.631562 67.737283 \nL 70.241509 62.246359 \nL 70.851456 67.737283 \nL 72.071351 59.500927 \nL 72.681298 63.93587 \nL 73.901192 84.210008 \nL 74.511139 92.446378 \nL 75.121086 105.527653 \nL 75.731034 111.664551 \nL 76.340981 111.953554 \nL 76.950928 117.155459 \nL 77.560875 117.155459 \nL 78.170822 121.648017 \nL 78.780769 121.452697 \nL 79.390717 129.510007 \nL 80.000664 129.015832 \nL 81.220558 120.816075 \nL 81.830505 127.745076 \nL 82.440452 123.971764 \nL 83.660347 129.908541 \nL 84.270294 129.510007 \nL 84.880241 123.145551 \nL 85.490188 122.969363 \nL 86.710083 133.628185 \nL 87.32003 135.854228 \nL 88.539924 145.03238 \nL 89.149871 144.335462 \nL 89.759819 141.261895 \nL 90.979713 144.737707 \nL 91.58966 148.603392 \nL 92.199607 147.904559 \nL 92.809554 145.087476 \nL 93.419502 144.493182 \nL 94.029449 139.805466 \nL 95.249343 142.852921 \nL 95.85929 142.34904 \nL 96.469237 143.765247 \nL 97.079185 141.398348 \nL 97.689132 144.610002 \nL 98.299079 142.313811 \nL 98.909026 143.629486 \nL 99.518973 146.632979 \nL 100.12892 144.420662 \nL 100.738868 138.932964 \nL 101.348815 135.275456 \nL 102.568709 134.690941 \nL 103.178656 135.981432 \nL 103.788603 132.598641 \nL 104.398551 133.88162 \nL 105.618445 139.282999 \nL 106.228392 140.411072 \nL 106.838339 140.074042 \nL 107.448286 142.570531 \nL 108.668181 144.610002 \nL 109.278128 142.879991 \nL 109.888075 142.532371 \nL 111.107969 144.465508 \nL 111.717917 146.678016 \nL 112.937811 148.432785 \nL 113.547758 145.570915 \nL 114.157705 143.999905 \nL 114.767652 146.083176 \nL 115.3776 145.734644 \nL 115.987547 146.571048 \nL 116.597494 148.550548 \nL 117.207441 145.886958 \nL 117.817388 144.420662 \nL 118.427335 146.357113 \nL 119.037283 146.029008 \nL 120.257177 147.566645 \nL 121.477071 144.787133 \nL 122.696966 146.286184 \nL 123.306913 144.953189 \nL 123.91686 146.704482 \nL 124.526807 145.394418 \nL 125.136754 147.105872 \nL 126.356649 146.5128 \nL 126.966596 145.255991 \nL 127.576543 146.902335 \nL 128.18649 145.665953 \nL 128.796437 146.335728 \nL 129.406384 147.925266 \nL 130.016332 148.561403 \nL 130.626279 150.100911 \nL 131.236226 149.798668 \nL 131.846173 150.400415 \nL 132.45612 150.100911 \nL 133.676015 153.016449 \nL 134.285962 153.568858 \nL 134.895909 154.971118 \nL 136.115803 152.635176 \nL 136.72575 152.334509 \nL 137.335698 153.699994 \nL 137.945645 154.219103 \nL 139.165539 153.611494 \nL 139.775486 154.922207 \nL 140.385433 154.617637 \nL 140.995381 153.527237 \nL 142.215275 154.510934 \nL 144.045116 153.648888 \nL 144.655064 154.879261 \nL 145.265011 153.095964 \nL 145.874958 153.568858 \nL 146.484905 150.346776 \nL 147.094852 150.833043 \nL 148.314747 153.227129 \nL 149.534641 151.28601 \nL 150.144588 152.454157 \nL 150.754535 151.502847 \nL 151.364482 151.956992 \nL 151.97443 153.095964 \nL 152.584377 152.846372 \nL 153.194324 153.281858 \nL 153.804271 153.034416 \nL 154.414218 153.4627 \nL 155.634113 150.322033 \nL 156.24406 149.442005 \nL 156.854007 149.882734 \nL 158.683848 149.245183 \nL 159.903743 150.100911 \nL 161.123637 152.186068 \nL 161.733584 151.34414 \nL 162.953479 152.147216 \nL 163.563426 151.931218 \nL 165.393267 153.095964 \nL 166.003214 152.879451 \nL 166.613162 153.257365 \nL 167.223109 153.042473 \nL 167.833056 153.999792 \nL 168.443003 154.364451 \nL 169.05295 153.568858 \nL 169.662897 153.931779 \nL 170.272845 153.719199 \nL 170.882792 154.077098 \nL 173.932528 158.567343 \nL 174.542475 157.788186 \nL 175.152422 157.563705 \nL 175.762369 157.884726 \nL 176.982263 159.590643 \nL 178.812105 160.495473 \nL 179.422052 160.264939 \nL 180.031999 159.513897 \nL 181.251894 160.10529 \nL 181.861841 159.88159 \nL 183.081735 160.460053 \nL 183.691682 159.224275 \nL 184.301629 159.513897 \nL 184.911577 158.295469 \nL 187.351365 159.44366 \nL 187.961312 157.766447 \nL 188.57126 158.540141 \nL 189.181207 158.821765 \nL 189.791154 158.618522 \nL 191.011048 159.172846 \nL 192.230943 158.770778 \nL 192.84089 159.513897 \nL 194.060784 159.114288 \nL 195.280678 159.645858 \nL 195.890626 159.448226 \nL 196.500573 159.710012 \nL 197.11052 159.513897 \nL 197.720467 158.412849 \nL 198.330414 157.773149 \nL 198.940361 158.037777 \nL 199.550309 157.852782 \nL 200.160256 158.114671 \nL 200.770203 157.487783 \nL 203.209992 156.77708 \nL 203.819939 156.16981 \nL 205.64978 155.663131 \nL 207.479622 156.436574 \nL 208.089569 156.269429 \nL 208.699516 156.522481 \nL 209.309463 156.356382 \nL 209.91941 156.606945 \nL 210.529358 157.268958 \nL 211.749252 157.756285 \nL 212.359199 157.180106 \nL 213.579093 156.85203 \nL 214.189041 157.093418 \nL 214.798988 156.127527 \nL 215.408935 155.969748 \nL 216.018882 155.414703 \nL 217.848724 156.138735 \nL 218.458671 155.984034 \nL 219.068618 156.22121 \nL 219.678565 156.845653 \nL 220.288512 156.302419 \nL 221.508407 156.766918 \nL 222.118354 155.847213 \nL 222.728301 155.69783 \nL 225.16809 156.614839 \nL 225.778037 155.716622 \nL 228.217825 156.616253 \nL 228.827773 156.46955 \nL 229.43772 155.591819 \nL 230.047667 155.085287 \nL 230.657614 155.309208 \nL 231.267561 155.169442 \nL 231.877508 155.391433 \nL 232.487456 155.252388 \nL 233.097403 155.472451 \nL 233.70735 156.047747 \nL 234.927244 156.477459 \nL 235.537191 157.04299 \nL 236.147139 157.252772 \nL 236.757086 156.760099 \nL 237.367033 157.31865 \nL 238.586927 157.730394 \nL 240.416769 157.307736 \nL 241.026716 156.826801 \nL 242.24661 157.233435 \nL 242.856557 156.757699 \nL 243.466505 155.947892 \nL 244.076452 155.815946 \nL 244.686399 155.014815 \nL 245.296346 155.220812 \nL 245.906293 155.758214 \nL 248.346082 155.245216 \nL 248.956029 154.464556 \nL 250.785871 155.069735 \nL 252.005765 154.178852 \nL 253.835606 154.77705 \nL 255.055501 154.535883 \nL 255.665448 154.732219 \nL 256.275395 153.353494 \nL 258.105237 153.946286 \nL 258.715184 153.519785 \nL 259.325131 153.71562 \nL 259.935078 153.601377 \nL 260.545025 153.179927 \nL 261.154972 153.374996 \nL 261.76492 153.874856 \nL 262.984814 153.040661 \nL 264.204708 153.425689 \nL 264.814655 153.315113 \nL 265.424603 153.505617 \nL 266.03455 153.095964 \nL 267.254444 152.879451 \nL 267.864391 152.475365 \nL 268.474338 152.961455 \nL 269.084286 153.149602 \nL 269.694233 153.042473 \nL 270.914127 151.075638 \nL 272.743969 151.646737 \nL 273.963863 151.445629 \nL 274.57381 151.633261 \nL 277.623546 151.139307 \nL 278.233493 150.195051 \nL 280.063335 150.75422 \nL 280.673282 150.38012 \nL 281.283229 150.564938 \nL 281.893176 150.471093 \nL 282.503123 150.100911 \nL 283.11307 150.008892 \nL 283.723018 149.366761 \nL 284.332965 149.55183 \nL 285.552859 150.464952 \nL 286.162806 150.64547 \nL 286.772753 150.553465 \nL 287.382701 150.732747 \nL 287.992648 150.370959 \nL 288.602595 150.280457 \nL 289.212542 150.459016 \nL 290.432436 150.278999 \nL 292.262278 150.807519 \nL 292.872225 150.717548 \nL 294.702067 151.236966 \nL 295.312014 151.146803 \nL 295.921961 151.57868 \nL 296.531908 151.488089 \nL 297.141855 151.6574 \nL 298.36175 152.50946 \nL 298.971697 152.417397 \nL 299.581644 151.812365 \nL 300.191591 151.466528 \nL 300.801538 151.633261 \nL 301.411485 151.544408 \nL 302.021433 151.96415 \nL 302.63138 151.621479 \nL 303.241327 152.038878 \nL 303.851274 151.697769 \nL 304.461221 151.609873 \nL 305.071168 151.020719 \nL 306.291063 149.352151 \nL 306.90101 149.768976 \nL 307.510957 149.687031 \nL 308.120904 150.100911 \nL 309.340799 150.429546 \nL 309.950746 150.838507 \nL 310.560693 151.000173 \nL 311.780587 150.833043 \nL 313.000482 151.638917 \nL 313.610429 151.312152 \nL 314.220376 151.470284 \nL 314.830323 151.868714 \nL 315.44027 152.02474 \nL 316.050217 151.940099 \nL 316.660165 152.334509 \nL 317.880059 152.164963 \nL 318.490006 151.84322 \nL 319.7099 152.150161 \nL 320.319848 151.830746 \nL 320.929795 151.748182 \nL 321.539742 151.900791 \nL 322.149689 151.584247 \nL 322.759636 151.736503 \nL 323.369583 152.121162 \nL 323.979531 152.038878 \nL 324.589478 152.189013 \nL 325.199425 151.644027 \nL 326.419319 151.944017 \nL 327.639214 151.323949 \nL 328.249161 151.24486 \nL 328.859108 151.622642 \nL 329.469055 151.54323 \nL 330.688949 152.29224 \nL 331.298897 152.211841 \nL 331.908844 152.357455 \nL 332.518791 152.727549 \nL 333.128738 152.646693 \nL 333.738685 153.01446 \nL 334.348632 152.709713 \nL 335.568527 152.994769 \nL 336.788421 153.72048 \nL 337.398368 153.638682 \nL 338.008315 153.33663 \nL 338.618263 153.696297 \nL 339.22821 153.834738 \nL 339.838157 153.753411 \nL 340.448104 153.891101 \nL 342.277946 152.997214 \nL 342.887893 152.918612 \nL 344.717734 153.330857 \nL 345.327681 153.681937 \nL 346.547576 152.668093 \nL 348.987365 152.363257 \nL 349.597312 152.499519 \nL 350.207259 152.423995 \nL 352.647048 152.962707 \nL 353.256995 153.304907 \nL 353.866942 153.228646 \nL 354.476889 153.360784 \nL 355.086836 153.284715 \nL 355.696783 153.00178 \nL 356.306731 152.513245 \nL 357.526625 152.365908 \nL 358.136572 152.498193 \nL 358.746519 152.424864 \nL 359.356466 152.556471 \nL 359.966414 152.891749 \nL 361.186308 152.337955 \nL 361.796255 152.468664 \nL 362.406202 152.396292 \nL 363.016149 152.526353 \nL 363.626097 152.85758 \nL 364.845991 153.114226 \nL 365.455938 153.04128 \nL 366.065885 153.168704 \nL 366.675832 153.095964 \nL 367.28578 153.422037 \nL 367.895727 153.150191 \nL 368.505674 152.482508 \nL 369.115621 152.609786 \nL 369.725568 152.934223 \nL 369.725568 152.934223 \n\" clip-path=\"url(#p98521e957c)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 65.361932 18.319091 \nL 65.971879 166.573636 \nL 66.581826 215.991821 \nL 67.191773 240.700909 \nL 67.80172 235.759096 \nL 68.411668 215.991821 \nL 69.021615 201.872342 \nL 69.631562 178.928184 \nL 70.241509 172.064545 \nL 70.851456 176.457278 \nL 71.461403 171.066209 \nL 72.071351 158.337281 \nL 72.681298 155.169442 \nL 73.291245 145.394418 \nL 73.901192 130.333643 \nL 75.121086 140.411072 \nL 75.731034 144.610002 \nL 76.340981 153.568858 \nL 76.950928 127.039101 \nL 77.560875 126.568446 \nL 78.170822 130.633147 \nL 78.780769 138.641634 \nL 80.000664 144.829637 \nL 80.610611 139.963849 \nL 81.830505 138.334678 \nL 82.440452 141.01251 \nL 83.660347 139.473355 \nL 84.270294 141.864555 \nL 84.880241 147.105872 \nL 85.490188 149.131927 \nL 86.100135 148.218319 \nL 86.710083 150.100911 \nL 87.32003 149.210499 \nL 87.929977 143.165032 \nL 88.539924 145.03238 \nL 89.149871 139.393649 \nL 89.759819 143.672536 \nL 90.369766 140.687925 \nL 91.58966 144.110834 \nL 92.199607 143.511826 \nL 92.809554 147.236095 \nL 93.419502 144.493182 \nL 95.85929 142.34904 \nL 96.469237 143.765247 \nL 97.079185 143.263178 \nL 97.689132 144.610002 \nL 98.299079 147.70488 \nL 100.12892 146.124738 \nL 100.738868 148.984119 \nL 101.958762 147.940554 \nL 102.568709 144.255755 \nL 103.788603 140.320231 \nL 104.398551 141.484417 \nL 105.008498 141.115796 \nL 106.228392 143.318024 \nL 107.448286 148.218319 \nL 108.058234 147.780816 \nL 109.278128 144.233914 \nL 110.498022 146.147463 \nL 111.107969 145.765984 \nL 111.717917 146.678016 \nL 112.327864 143.765247 \nL 112.937811 145.930611 \nL 113.547758 145.570915 \nL 114.157705 147.660505 \nL 114.767652 148.493817 \nL 115.3776 148.116256 \nL 117.207441 150.484009 \nL 117.817388 148.96487 \nL 118.427335 149.726531 \nL 119.64723 149.002735 \nL 120.257177 150.824987 \nL 120.867124 151.533333 \nL 121.477071 150.100911 \nL 123.306913 152.16 \nL 123.91686 149.761274 \nL 124.526807 149.428559 \nL 125.136754 151.099262 \nL 125.746701 151.748182 \nL 126.966596 151.069895 \nL 128.18649 148.516998 \nL 128.796437 149.159615 \nL 130.626279 145.525156 \nL 131.236226 145.264887 \nL 131.846173 145.907857 \nL 132.45612 145.648825 \nL 133.066067 146.276891 \nL 133.676015 145.144517 \nL 134.895909 146.376642 \nL 135.505856 147.828814 \nL 136.72575 148.984119 \nL 137.945645 146.806369 \nL 138.555592 146.561328 \nL 139.165539 147.130424 \nL 139.775486 146.886723 \nL 140.385433 147.444022 \nL 140.995381 148.7831 \nL 141.605328 149.316495 \nL 142.825222 148.813984 \nL 143.435169 149.334743 \nL 144.045116 148.326937 \nL 146.484905 150.346776 \nL 147.094852 150.100911 \nL 147.704799 150.58541 \nL 148.314747 149.619961 \nL 149.534641 151.997052 \nL 150.144588 151.042221 \nL 150.754535 151.502847 \nL 151.364482 152.653026 \nL 151.97443 151.71363 \nL 152.584377 151.473642 \nL 153.194324 149.192075 \nL 153.804271 148.972646 \nL 155.024165 149.878315 \nL 155.634113 149.658695 \nL 156.854007 150.53728 \nL 158.073901 148.808932 \nL 158.683848 148.603392 \nL 159.293796 149.038155 \nL 159.903743 147.566645 \nL 161.733584 146.992852 \nL 162.343531 147.424095 \nL 162.953479 146.622198 \nL 164.173373 147.47336 \nL 164.78332 147.288497 \nL 165.393267 146.506865 \nL 166.003214 146.330043 \nL 167.223109 147.159364 \nL 167.833056 145.812156 \nL 168.443003 145.643597 \nL 169.05295 146.632979 \nL 170.272845 146.292193 \nL 170.882792 145.556718 \nL 172.102686 145.233973 \nL 172.712633 145.633729 \nL 173.32258 145.473741 \nL 174.542475 146.257273 \nL 175.152422 146.09649 \nL 175.762369 146.480531 \nL 176.372316 147.400472 \nL 176.982263 146.698944 \nL 177.592211 147.60775 \nL 178.202158 147.444022 \nL 179.422052 146.070348 \nL 180.031999 146.44031 \nL 181.251894 146.133663 \nL 181.861841 147.012277 \nL 182.471788 146.857577 \nL 183.691682 145.539236 \nL 184.911577 146.25449 \nL 185.521524 146.107521 \nL 186.131471 146.458705 \nL 187.351365 145.183678 \nL 187.961312 145.044928 \nL 188.57126 145.881303 \nL 189.181207 145.740491 \nL 190.401101 146.422534 \nL 191.011048 146.281147 \nL 192.230943 146.948241 \nL 192.84089 146.806369 \nL 193.450837 147.602684 \nL 194.060784 147.459059 \nL 194.670731 146.852776 \nL 195.280678 145.328437 \nL 195.890626 145.197404 \nL 196.500573 145.525156 \nL 197.11052 143.572549 \nL 197.720467 143.904748 \nL 198.330414 143.331308 \nL 199.550309 144.883305 \nL 201.990097 144.390367 \nL 202.600044 144.707191 \nL 203.209992 143.715011 \nL 203.819939 144.465508 \nL 205.64978 145.394418 \nL 206.259727 145.272707 \nL 206.869675 145.576217 \nL 208.089569 147.016666 \nL 209.309463 147.598722 \nL 209.91941 148.301369 \nL 210.529358 147.757517 \nL 211.139305 148.041822 \nL 211.749252 147.913661 \nL 212.359199 148.194976 \nL 212.969146 148.067242 \nL 213.579093 146.725351 \nL 214.189041 146.604657 \nL 216.018882 147.444022 \nL 216.628829 146.925457 \nL 217.238776 146.015679 \nL 217.848724 146.294476 \nL 218.458671 146.178833 \nL 219.068618 146.454773 \nL 219.678565 146.339425 \nL 220.288512 146.612581 \nL 220.898459 146.497513 \nL 221.508407 145.614185 \nL 222.118354 146.270042 \nL 222.728301 146.539251 \nL 223.338248 146.42623 \nL 224.558142 145.448307 \nL 226.997931 146.509118 \nL 227.607878 146.029008 \nL 228.217825 145.183678 \nL 229.43772 145.708193 \nL 230.047667 146.332252 \nL 231.877508 144.93064 \nL 232.487456 144.829637 \nL 233.097403 145.087476 \nL 233.70735 144.986635 \nL 234.317297 144.175474 \nL 234.927244 144.078624 \nL 235.537191 144.688442 \nL 236.147139 143.887001 \nL 236.757086 144.14269 \nL 237.367033 143.348849 \nL 237.97698 143.256609 \nL 238.586927 142.818237 \nL 239.196874 142.728501 \nL 240.416769 143.237286 \nL 241.026716 142.805041 \nL 241.636663 142.716586 \nL 242.24661 142.968401 \nL 242.856557 142.879991 \nL 244.076452 144.049714 \nL 244.686399 144.293576 \nL 245.296346 143.867988 \nL 245.906293 143.112483 \nL 246.51624 143.025383 \nL 247.126188 142.608307 \nL 247.736135 142.852921 \nL 248.346082 143.424255 \nL 249.565976 143.903261 \nL 250.175923 143.490147 \nL 250.785871 143.727868 \nL 252.615712 145.394418 \nL 253.225659 145.62289 \nL 253.835606 145.531062 \nL 254.445554 145.757619 \nL 255.055501 145.665953 \nL 256.275395 146.113884 \nL 256.885342 146.649488 \nL 257.495289 146.868932 \nL 258.105237 146.77519 \nL 258.715184 146.992852 \nL 259.935078 145.570915 \nL 260.545025 145.174488 \nL 261.154972 145.394418 \nL 261.76492 144.389012 \nL 262.374867 143.999905 \nL 263.594761 144.441575 \nL 264.204708 143.753627 \nL 266.03455 143.511826 \nL 266.644497 144.029404 \nL 267.254444 143.353046 \nL 267.864391 143.571179 \nL 268.474338 142.01251 \nL 269.084286 142.52838 \nL 269.694233 142.747014 \nL 270.30418 142.377803 \nL 271.524074 142.228994 \nL 272.134021 142.445949 \nL 272.743969 142.081937 \nL 274.57381 142.726498 \nL 275.183757 143.225342 \nL 275.793704 142.57869 \nL 276.403652 142.505419 \nL 277.013599 142.148565 \nL 278.84344 141.934954 \nL 279.453387 142.426125 \nL 280.063335 142.354533 \nL 281.893176 142.975073 \nL 282.503123 142.902745 \nL 283.11307 143.382988 \nL 283.723018 143.034617 \nL 284.332965 142.962731 \nL 284.942912 143.438821 \nL 285.552859 142.820152 \nL 287.992648 143.61984 \nL 288.602595 144.086356 \nL 289.212542 144.013174 \nL 289.822489 144.476083 \nL 290.432436 144.66937 \nL 291.652331 144.521444 \nL 292.262278 144.713052 \nL 293.482172 144.566084 \nL 294.092119 144.23032 \nL 294.702067 144.420662 \nL 295.312014 144.34854 \nL 296.531908 144.7256 \nL 297.141855 145.172073 \nL 297.751802 145.357466 \nL 299.581644 144.367554 \nL 300.191591 144.553109 \nL 300.801538 144.993086 \nL 301.411485 144.921347 \nL 302.021433 144.595893 \nL 302.63138 144.778959 \nL 303.241327 143.949977 \nL 304.461221 144.316596 \nL 305.071168 144.24767 \nL 306.291063 145.109185 \nL 306.90101 145.038772 \nL 307.510957 144.720372 \nL 308.730851 145.07674 \nL 309.950746 144.937828 \nL 310.560693 144.623625 \nL 311.17064 144.800285 \nL 311.780587 144.732022 \nL 312.390534 144.907547 \nL 313.000482 145.325035 \nL 313.610429 145.255991 \nL 314.220376 145.670593 \nL 314.830323 145.359984 \nL 315.44027 144.810403 \nL 316.050217 144.983174 \nL 317.270112 144.371265 \nL 319.099953 144.886531 \nL 319.7099 144.820182 \nL 320.319848 144.990052 \nL 320.929795 144.453122 \nL 321.539742 144.38829 \nL 322.759636 145.19415 \nL 323.369583 144.894911 \nL 323.979531 144.829637 \nL 324.589478 144.996694 \nL 325.809372 145.7903 \nL 327.029266 145.657117 \nL 328.249161 146.44031 \nL 328.859108 146.600931 \nL 330.079002 146.465553 \nL 330.688949 146.171646 \nL 331.298897 146.331413 \nL 331.908844 146.264799 \nL 333.128738 145.683229 \nL 334.95858 146.159363 \nL 335.568527 146.094031 \nL 336.178474 146.251117 \nL 336.788421 145.299453 \nL 337.398368 145.015369 \nL 338.008315 145.394418 \nL 338.618263 145.551651 \nL 339.22821 145.927828 \nL 341.058051 145.73728 \nL 341.667998 146.109731 \nL 342.277946 146.046092 \nL 343.49784 146.352194 \nL 344.107787 146.720049 \nL 344.717734 146.44031 \nL 345.937629 146.742052 \nL 347.157523 146.614245 \nL 347.76747 146.763761 \nL 348.377417 146.700093 \nL 348.987365 146.848785 \nL 349.597312 146.573567 \nL 350.817206 146.869595 \nL 352.0371 146.743422 \nL 352.647048 146.890125 \nL 353.256995 146.200395 \nL 353.866942 146.347643 \nL 354.476889 146.702332 \nL 355.696783 146.578442 \nL 356.306731 146.310116 \nL 356.916678 145.836575 \nL 357.526625 145.982733 \nL 358.136572 145.717324 \nL 359.966414 146.152912 \nL 361.796255 145.974279 \nL 362.406202 145.712685 \nL 363.016149 145.654289 \nL 363.626097 145.394418 \nL 364.236044 144.934322 \nL 364.845991 144.676969 \nL 365.455938 144.621151 \nL 366.065885 144.765616 \nL 366.675832 144.709842 \nL 368.505674 145.139244 \nL 369.115621 145.083176 \nL 369.725568 144.631976 \nL 369.725568 144.631976 \n\" clip-path=\"url(#p98521e957c)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 65.361932 215.991821 \nL 65.971879 215.991821 \nL 66.581826 150.100911 \nL 67.191773 166.573636 \nL 67.80172 176.457278 \nL 68.411668 199.519095 \nL 69.021615 201.872342 \nL 69.631562 203.63728 \nL 70.241509 194.028186 \nL 70.851456 186.340919 \nL 71.461403 198.021576 \nL 72.681298 185.580627 \nL 73.291245 180.693115 \nL 73.901192 183.046362 \nL 74.511139 166.573636 \nL 75.121086 169.480588 \nL 75.731034 155.591819 \nL 76.340981 148.366937 \nL 76.950928 151.748182 \nL 77.560875 159.513897 \nL 78.170822 157.588521 \nL 78.780769 164.425032 \nL 79.390717 162.455459 \nL 80.000664 152.736547 \nL 81.220558 164.743343 \nL 82.440452 168.277712 \nL 83.0504 173.162735 \nL 84.270294 169.662284 \nL 85.490188 172.38754 \nL 86.710083 169.319098 \nL 87.32003 173.25178 \nL 88.539924 165.306504 \nL 89.149871 164.10273 \nL 89.759819 165.368316 \nL 90.369766 161.867143 \nL 90.979713 163.125867 \nL 91.58966 162.081079 \nL 92.199607 165.475461 \nL 92.809554 166.573636 \nL 93.419502 163.419288 \nL 94.029449 164.514548 \nL 94.639396 163.548038 \nL 95.249343 160.643457 \nL 95.85929 161.728717 \nL 96.469237 158.97084 \nL 97.079185 160.046718 \nL 97.689132 153.761526 \nL 98.299079 154.892987 \nL 98.909026 152.454157 \nL 99.518973 151.834884 \nL 100.12892 152.941042 \nL 100.738868 152.334509 \nL 101.348815 153.395468 \nL 102.568709 152.226422 \nL 103.178656 154.807404 \nL 103.788603 155.763412 \nL 104.398551 153.648888 \nL 105.008498 154.593483 \nL 106.838339 148.668503 \nL 107.448286 149.63027 \nL 108.058234 147.780816 \nL 108.668181 150.100911 \nL 109.278128 151.003531 \nL 109.888075 150.546131 \nL 110.498022 151.418736 \nL 111.107969 148.366937 \nL 112.327864 152.635176 \nL 112.937811 152.186068 \nL 114.157705 153.761526 \nL 114.767652 152.109778 \nL 115.3776 152.879451 \nL 115.987547 152.454157 \nL 116.597494 153.201665 \nL 117.207441 155.081046 \nL 117.817388 153.509063 \nL 118.427335 150.84967 \nL 119.037283 151.581611 \nL 119.64723 151.199101 \nL 121.477071 153.289192 \nL 122.087018 155.007687 \nL 122.696966 153.568858 \nL 123.306913 153.189559 \nL 123.91686 153.836991 \nL 124.526807 155.47977 \nL 125.746701 154.713279 \nL 126.356649 156.298575 \nL 126.966596 153.976846 \nL 127.576543 153.619359 \nL 128.18649 154.219103 \nL 128.796437 153.866108 \nL 129.406384 154.4522 \nL 130.626279 153.761526 \nL 131.236226 154.332448 \nL 131.846173 152.197438 \nL 132.45612 153.662585 \nL 133.066067 153.33663 \nL 133.676015 152.141781 \nL 134.285962 153.568858 \nL 134.895909 154.111664 \nL 136.115803 153.479931 \nL 136.72575 154.847301 \nL 137.335698 155.361109 \nL 137.945645 156.69001 \nL 139.775486 158.136394 \nL 140.385433 157.805903 \nL 140.995381 155.89932 \nL 141.605328 156.376235 \nL 142.215275 156.067408 \nL 143.435169 158.528815 \nL 144.045116 158.210563 \nL 145.265011 159.08604 \nL 145.874958 158.027644 \nL 147.094852 158.886376 \nL 147.704799 157.852782 \nL 148.314747 158.998587 \nL 148.924694 157.262964 \nL 149.534641 156.263376 \nL 150.144588 156.69001 \nL 150.754535 155.708655 \nL 151.97443 155.169442 \nL 152.584377 155.591819 \nL 153.194324 155.326749 \nL 153.804271 155.742263 \nL 155.024165 155.220812 \nL 155.634113 155.628683 \nL 156.24406 155.372184 \nL 156.854007 156.428194 \nL 157.463954 156.820056 \nL 158.073901 155.914814 \nL 158.683848 156.304923 \nL 159.293796 155.414703 \nL 159.903743 155.803015 \nL 161.123637 155.313803 \nL 161.733584 156.317044 \nL 162.343531 156.072283 \nL 162.953479 157.058337 \nL 163.563426 156.812029 \nL 164.173373 157.175098 \nL 164.78332 156.931074 \nL 165.393267 157.289017 \nL 166.003214 155.856448 \nL 166.613162 155.624706 \nL 167.223109 155.984034 \nL 168.443003 157.852782 \nL 169.05295 157.036805 \nL 169.662897 157.379564 \nL 170.272845 156.575752 \nL 170.882792 155.213139 \nL 171.492739 154.995669 \nL 172.102686 154.219103 \nL 173.32258 154.913179 \nL 175.152422 154.287352 \nL 176.372316 152.801364 \nL 176.982263 153.144786 \nL 177.592211 152.415998 \nL 178.812105 152.038878 \nL 179.422052 152.37906 \nL 180.031999 151.669742 \nL 180.641946 150.447705 \nL 181.251894 150.273402 \nL 182.471788 148.906003 \nL 183.081735 149.251811 \nL 184.301629 148.924295 \nL 184.911577 149.264742 \nL 185.521524 147.605055 \nL 186.741418 149.277275 \nL 187.351365 149.609195 \nL 187.961312 150.427101 \nL 188.57126 148.80257 \nL 189.181207 148.162943 \nL 189.791154 148.975945 \nL 190.401101 148.82148 \nL 191.011048 148.191029 \nL 191.620995 148.041822 \nL 192.230943 148.366937 \nL 192.84089 148.218319 \nL 194.060784 146.992852 \nL 194.670731 147.316789 \nL 195.280678 148.099555 \nL 195.890626 147.955635 \nL 196.500573 146.44031 \nL 197.720467 147.07839 \nL 198.330414 147.844381 \nL 198.940361 147.255624 \nL 199.550309 147.119422 \nL 201.38015 148.041822 \nL 201.990097 147.465274 \nL 202.600044 148.20583 \nL 203.209992 148.069039 \nL 203.819939 148.366937 \nL 205.039833 147.236095 \nL 206.259727 147.828814 \nL 206.869675 147.697177 \nL 207.479622 148.4114 \nL 208.089569 147.857828 \nL 209.309463 149.266848 \nL 209.91941 149.131927 \nL 211.139305 150.512729 \nL 212.359199 150.237054 \nL 213.579093 151.586162 \nL 214.798988 152.109778 \nL 215.408935 151.968274 \nL 216.628829 153.276379 \nL 218.458671 154.022988 \nL 219.068618 153.095964 \nL 219.678565 152.565337 \nL 220.288512 152.426484 \nL 220.898459 152.674779 \nL 222.728301 152.263359 \nL 226.387984 153.706268 \nL 226.997931 152.825724 \nL 227.607878 153.062311 \nL 228.217825 152.928318 \nL 228.827773 153.162754 \nL 229.43772 153.761526 \nL 230.657614 153.492362 \nL 231.267561 154.083328 \nL 232.487456 154.53357 \nL 233.70735 153.55008 \nL 234.317297 154.130221 \nL 235.537191 154.572084 \nL 236.147139 154.43893 \nL 237.97698 155.089131 \nL 238.586927 154.609242 \nL 239.196874 154.823869 \nL 239.806822 155.381374 \nL 240.416769 154.562275 \nL 241.026716 155.116834 \nL 242.856557 153.711378 \nL 244.686399 154.344731 \nL 245.296346 154.219103 \nL 245.906293 153.761526 \nL 246.51624 153.638682 \nL 247.126188 153.847227 \nL 247.736135 153.724913 \nL 248.346082 154.260149 \nL 248.956029 154.137276 \nL 249.565976 153.689036 \nL 250.175923 153.568858 \nL 250.785871 153.125419 \nL 252.005765 153.534969 \nL 252.615712 152.454157 \nL 253.225659 152.339929 \nL 254.445554 152.749257 \nL 255.055501 152.635176 \nL 255.665448 152.206053 \nL 256.275395 152.409193 \nL 256.885342 152.297277 \nL 257.495289 152.498841 \nL 258.715184 153.519785 \nL 260.545025 154.103638 \nL 261.154972 153.988893 \nL 261.76492 154.180855 \nL 262.374867 154.67668 \nL 262.984814 154.257116 \nL 263.594761 154.749656 \nL 264.814655 155.123093 \nL 265.424603 155.007687 \nL 266.644497 155.376175 \nL 267.864391 155.146614 \nL 268.474338 155.328781 \nL 269.084286 154.624765 \nL 269.694233 154.807404 \nL 270.914127 155.754281 \nL 271.524074 155.931972 \nL 272.134021 155.527223 \nL 272.743969 155.414703 \nL 273.353916 155.013828 \nL 273.963863 154.903458 \nL 274.57381 155.368355 \nL 275.183757 155.257602 \nL 275.793704 155.718787 \nL 277.013599 156.065184 \nL 277.623546 155.387295 \nL 278.84344 156.295792 \nL 280.063335 156.074036 \nL 280.673282 156.243287 \nL 281.283229 156.133182 \nL 281.893176 156.301329 \nL 282.503123 156.191666 \nL 283.11307 156.358709 \nL 283.723018 156.800129 \nL 284.332965 156.415455 \nL 285.552859 156.198559 \nL 286.162806 155.546443 \nL 286.772753 155.984034 \nL 287.382701 155.877656 \nL 287.992648 156.0419 \nL 288.602595 155.935948 \nL 289.212542 155.293405 \nL 289.822489 155.4579 \nL 290.432436 155.087261 \nL 291.042384 154.985006 \nL 291.652331 155.149014 \nL 292.262278 154.517193 \nL 292.872225 154.68157 \nL 294.092119 154.481964 \nL 294.702067 154.120781 \nL 295.312014 154.545942 \nL 295.921961 154.186496 \nL 296.531908 154.349149 \nL 297.751802 153.636944 \nL 298.971697 153.961706 \nL 299.581644 153.866108 \nL 300.191591 154.027053 \nL 300.801538 154.442568 \nL 302.63138 153.395468 \nL 303.851274 153.210561 \nL 304.461221 153.621804 \nL 305.071168 153.529255 \nL 305.681116 152.936727 \nL 306.291063 153.095964 \nL 306.90101 152.507516 \nL 307.510957 152.915357 \nL 308.730851 152.736547 \nL 309.340799 152.894297 \nL 309.950746 152.559534 \nL 310.560693 152.471683 \nL 311.780587 153.273448 \nL 312.390534 152.697607 \nL 313.000482 152.36744 \nL 313.610429 152.523378 \nL 314.220376 152.436896 \nL 314.830323 152.109778 \nL 315.44027 152.265215 \nL 316.050217 152.18 \nL 316.660165 151.61656 \nL 317.270112 151.533333 \nL 317.880059 151.212327 \nL 318.490006 151.130455 \nL 319.7099 151.440799 \nL 320.319848 151.123091 \nL 320.929795 151.042221 \nL 321.539742 151.196494 \nL 322.149689 151.584247 \nL 322.759636 151.736503 \nL 323.369583 151.65494 \nL 323.979531 151.806327 \nL 324.589478 151.260973 \nL 325.199425 151.412566 \nL 325.809372 151.33252 \nL 326.419319 151.71363 \nL 327.029266 151.633261 \nL 327.639214 151.782586 \nL 328.249161 151.24486 \nL 328.859108 150.93786 \nL 329.469055 150.860024 \nL 331.298897 151.985667 \nL 331.908844 152.131796 \nL 332.518791 151.82699 \nL 333.128738 151.97281 \nL 333.738685 152.342108 \nL 334.348632 152.486102 \nL 334.95858 152.18324 \nL 336.178474 152.025815 \nL 337.398368 152.31202 \nL 338.008315 152.674779 \nL 338.618263 152.155419 \nL 339.22821 152.077642 \nL 339.838157 151.781054 \nL 341.058051 151.628195 \nL 341.667998 150.899156 \nL 342.277946 150.607767 \nL 342.887893 150.967905 \nL 343.49784 150.893913 \nL 344.107787 150.604453 \nL 345.937629 150.386777 \nL 346.547576 150.742717 \nL 347.157523 150.670168 \nL 347.76747 151.023959 \nL 348.377417 151.163666 \nL 348.987365 151.09069 \nL 349.597312 151.229661 \nL 350.207259 151.156862 \nL 350.817206 150.662878 \nL 351.427153 151.012176 \nL 352.0371 150.94029 \nL 352.647048 151.078113 \nL 353.256995 151.424304 \nL 355.086836 151.208321 \nL 356.306731 151.479385 \nL 356.916678 151.820407 \nL 357.526625 151.748182 \nL 359.356466 152.761113 \nL 359.966414 152.279132 \nL 360.576361 152.206701 \nL 361.186308 152.541316 \nL 361.796255 152.671613 \nL 362.406202 152.598828 \nL 363.016149 152.324229 \nL 363.626097 151.849038 \nL 364.236044 152.180972 \nL 365.455938 152.038878 \nL 366.065885 151.368043 \nL 366.675832 151.498605 \nL 368.505674 150.69631 \nL 369.115621 150.827166 \nL 369.725568 150.759831 \nL 369.725568 150.759831 \n\" clip-path=\"url(#p98521e957c)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 65.361932 117.155459 \nL 65.971879 67.737283 \nL 66.581826 150.100911 \nL 67.191773 191.282732 \nL 67.80172 196.224553 \nL 68.411668 183.046362 \nL 69.021615 145.394418 \nL 69.631562 117.155459 \nL 70.241509 128.137277 \nL 70.851456 117.155459 \nL 72.071351 133.628185 \nL 72.681298 132.361052 \nL 73.291245 138.334678 \nL 74.511139 135.687274 \nL 75.121086 117.155459 \nL 76.340981 117.155459 \nL 76.950928 127.039101 \nL 77.560875 117.155459 \nL 78.170822 112.662902 \nL 78.780769 112.858236 \nL 79.390717 121.273637 \nL 80.000664 125.06237 \nL 80.610611 120.956858 \nL 81.220558 128.137277 \nL 81.830505 124.215199 \nL 82.440452 130.788068 \nL 83.0504 120.450002 \nL 83.660347 120.343726 \nL 84.880241 126.140589 \nL 85.490188 122.969363 \nL 86.100135 117.155459 \nL 86.710083 122.646368 \nL 87.32003 122.497971 \nL 87.929977 124.958333 \nL 88.539924 124.758256 \nL 90.369766 138.334678 \nL 90.979713 137.842137 \nL 91.58966 139.618277 \nL 92.199607 136.922727 \nL 92.809554 136.493015 \nL 93.419502 138.18447 \nL 94.639396 137.32615 \nL 95.249343 140.87619 \nL 95.85929 140.411072 \nL 97.079185 143.263178 \nL 97.689132 142.779709 \nL 98.299079 138.719765 \nL 98.909026 138.334678 \nL 100.738868 142.283354 \nL 101.348815 143.511826 \nL 101.958762 143.079758 \nL 102.568709 144.255755 \nL 103.178656 143.825587 \nL 104.398551 149.087213 \nL 105.008498 148.603392 \nL 105.618445 145.183678 \nL 106.228392 143.318024 \nL 107.448286 142.570531 \nL 108.058234 144.996694 \nL 109.278128 146.941761 \nL 109.888075 149.210499 \nL 110.498022 148.7831 \nL 111.107969 149.667428 \nL 111.717917 149.245183 \nL 112.327864 150.100911 \nL 113.547758 149.277275 \nL 114.157705 151.321121 \nL 114.767652 146.083176 \nL 115.987547 147.747664 \nL 116.597494 143.899417 \nL 117.207441 143.58844 \nL 117.817388 144.420662 \nL 118.427335 144.110834 \nL 119.037283 144.91849 \nL 120.257177 144.308304 \nL 121.477071 145.849889 \nL 122.087018 145.544626 \nL 122.696966 144.205416 \nL 124.526807 146.40296 \nL 125.136754 145.109185 \nL 125.746701 144.829637 \nL 126.356649 143.57707 \nL 128.18649 148.516998 \nL 129.406384 146.060437 \nL 130.016332 145.7903 \nL 131.236226 147.07839 \nL 131.846173 145.907857 \nL 132.45612 142.977577 \nL 133.676015 144.269864 \nL 134.285962 144.032011 \nL 134.895909 142.938857 \nL 135.505856 144.420662 \nL 136.72575 145.633729 \nL 137.945645 145.159097 \nL 138.555592 144.110834 \nL 140.385433 143.458689 \nL 140.995381 142.457569 \nL 141.605328 140.687925 \nL 142.825222 138.775922 \nL 144.045116 138.443295 \nL 144.655064 137.526315 \nL 146.484905 137.070255 \nL 147.704799 135.323911 \nL 148.314747 135.191302 \nL 148.924694 136.493015 \nL 149.534641 136.353897 \nL 150.144588 135.510791 \nL 150.754535 136.782534 \nL 151.97443 137.890356 \nL 153.194324 140.330879 \nL 153.804271 140.172144 \nL 154.414218 141.360291 \nL 155.634113 142.362044 \nL 156.24406 142.194001 \nL 156.854007 141.373649 \nL 158.073901 143.641018 \nL 158.683848 142.185444 \nL 159.293796 142.661622 \nL 159.903743 143.765247 \nL 160.51369 143.59576 \nL 161.123637 144.05397 \nL 161.733584 145.128022 \nL 162.343531 143.717735 \nL 163.563426 142.169597 \nL 164.173373 139.590706 \nL 164.78332 139.453915 \nL 165.393267 139.91778 \nL 166.003214 139.78065 \nL 166.613162 140.237005 \nL 167.223109 140.099624 \nL 167.833056 139.379024 \nL 168.443003 137.504121 \nL 169.05295 137.38512 \nL 169.662897 138.416771 \nL 170.882792 139.308434 \nL 171.492739 138.617068 \nL 172.102686 138.495137 \nL 172.712633 139.491367 \nL 174.542475 139.119094 \nL 175.152422 140.08986 \nL 175.762369 140.506906 \nL 176.372316 139.299126 \nL 176.982263 139.178785 \nL 177.592211 138.525491 \nL 178.812105 138.296931 \nL 179.422052 137.658747 \nL 180.641946 139.523695 \nL 181.251894 139.924054 \nL 181.861841 140.835011 \nL 182.471788 140.712314 \nL 183.691682 141.484417 \nL 184.301629 140.856013 \nL 184.911577 140.735716 \nL 185.521524 141.115796 \nL 186.131471 140.995381 \nL 186.741418 140.382014 \nL 188.57126 141.499395 \nL 189.181207 141.380056 \nL 189.791154 141.744023 \nL 190.401101 140.665082 \nL 191.011048 141.028976 \nL 191.620995 140.914202 \nL 192.230943 140.327625 \nL 193.450837 140.107975 \nL 194.060784 140.465918 \nL 194.670731 141.284532 \nL 195.280678 141.171776 \nL 196.500573 141.864555 \nL 197.11052 142.661622 \nL 198.330414 143.331308 \nL 198.940361 142.313811 \nL 199.550309 140.858295 \nL 201.38015 139.217151 \nL 201.990097 139.997649 \nL 203.209992 140.667203 \nL 203.819939 140.130582 \nL 204.429886 140.893451 \nL 205.039833 141.219965 \nL 205.64978 140.687925 \nL 206.259727 141.43854 \nL 206.869675 141.758501 \nL 207.479622 140.808604 \nL 208.089569 141.549116 \nL 208.699516 140.608159 \nL 209.309463 140.926235 \nL 209.91941 140.826351 \nL 210.529358 141.140862 \nL 211.139305 140.629102 \nL 211.749252 140.531693 \nL 212.359199 141.251924 \nL 213.579093 141.864555 \nL 214.189041 141.360291 \nL 214.798988 141.261895 \nL 216.018882 141.864555 \nL 216.628829 140.971449 \nL 217.238776 141.271527 \nL 217.848724 140.38792 \nL 219.678565 140.113512 \nL 220.288512 140.411072 \nL 220.898459 140.320231 \nL 221.508407 140.999255 \nL 222.118354 141.289922 \nL 222.728301 140.815128 \nL 223.948195 141.391205 \nL 224.558142 140.921463 \nL 225.16809 140.831093 \nL 225.778037 141.115796 \nL 226.387984 141.025382 \nL 226.997931 141.307198 \nL 228.217825 142.602136 \nL 228.827773 142.50754 \nL 229.43772 142.779709 \nL 230.657614 142.591297 \nL 231.267561 141.774038 \nL 232.487456 141.594993 \nL 233.097403 141.148344 \nL 233.70735 140.348111 \nL 234.317297 140.975735 \nL 235.537191 140.099624 \nL 236.147139 140.72143 \nL 236.757086 140.287374 \nL 237.367033 140.205635 \nL 237.97698 140.472487 \nL 238.586927 140.390674 \nL 239.196874 139.963849 \nL 239.806822 140.228757 \nL 240.416769 140.148638 \nL 241.026716 140.753066 \nL 241.636663 141.01251 \nL 242.24661 140.930535 \nL 242.856557 141.526067 \nL 243.466505 141.780224 \nL 244.076452 140.687925 \nL 247.126188 141.947193 \nL 247.736135 141.535095 \nL 249.565976 142.272293 \nL 250.175923 141.864555 \nL 250.785871 142.107593 \nL 252.005765 141.945043 \nL 252.615712 142.506347 \nL 253.835606 141.705142 \nL 254.445554 142.261807 \nL 255.055501 142.498114 \nL 255.665448 142.101378 \nL 256.275395 142.336698 \nL 257.495289 142.177329 \nL 258.105237 141.474814 \nL 258.715184 141.087533 \nL 259.325131 141.322353 \nL 259.935078 141.864555 \nL 260.545025 141.787573 \nL 261.154972 142.018019 \nL 261.76492 141.941051 \nL 262.374867 142.474653 \nL 262.984814 142.396743 \nL 263.594761 141.712962 \nL 264.204708 141.637866 \nL 264.814655 141.261895 \nL 265.424603 141.789444 \nL 267.254444 141.566848 \nL 269.084286 142.233339 \nL 269.694233 142.747014 \nL 270.30418 142.964366 \nL 270.914127 143.472842 \nL 271.524074 143.686763 \nL 272.134021 143.60872 \nL 272.743969 144.110834 \nL 273.353916 144.321014 \nL 275.183757 144.084795 \nL 275.793704 144.292619 \nL 276.403652 143.929579 \nL 277.013599 143.852642 \nL 277.623546 144.059346 \nL 278.233493 143.982482 \nL 278.84344 143.624464 \nL 279.453387 144.110834 \nL 280.063335 144.31446 \nL 280.673282 144.237743 \nL 281.893176 143.530339 \nL 282.503123 144.010155 \nL 283.723018 143.86055 \nL 284.332965 144.335462 \nL 284.942912 144.260174 \nL 285.552859 143.639236 \nL 286.162806 143.838562 \nL 286.772753 143.765247 \nL 287.382701 143.42156 \nL 287.992648 143.889888 \nL 289.212542 143.744599 \nL 289.822489 143.404682 \nL 290.432436 143.60087 \nL 291.042384 143.263178 \nL 292.262278 143.653139 \nL 292.872225 143.582299 \nL 293.482172 143.248258 \nL 295.312014 143.825587 \nL 295.921961 143.494433 \nL 296.531908 142.904939 \nL 297.141855 142.837354 \nL 297.751802 143.028859 \nL 298.36175 143.477422 \nL 298.971697 143.408865 \nL 300.191591 144.297052 \nL 300.801538 143.716145 \nL 302.63138 144.272103 \nL 303.241327 144.20275 \nL 303.851274 144.38589 \nL 304.461221 144.316596 \nL 305.681116 145.179952 \nL 306.291063 145.109185 \nL 306.90101 145.28773 \nL 307.510957 145.217036 \nL 308.120904 145.394418 \nL 308.730851 145.323828 \nL 309.340799 145.50006 \nL 309.950746 145.429543 \nL 310.560693 145.114134 \nL 311.17064 145.044928 \nL 311.780587 144.732022 \nL 312.390534 144.664097 \nL 313.000482 143.867988 \nL 313.610429 144.044765 \nL 314.220376 143.495714 \nL 314.830323 143.9136 \nL 315.44027 143.608013 \nL 316.050217 143.064029 \nL 316.660165 143.2406 \nL 318.490006 143.052482 \nL 319.099953 143.464418 \nL 319.7099 143.165032 \nL 320.319848 143.102954 \nL 321.539742 143.918755 \nL 322.149689 143.855337 \nL 323.979531 144.364535 \nL 324.589478 144.068654 \nL 325.199425 144.468557 \nL 325.809372 143.942893 \nL 326.419319 143.880448 \nL 327.029266 144.277994 \nL 327.639214 143.985751 \nL 328.249161 144.381221 \nL 328.859108 144.546599 \nL 330.079002 143.966252 \nL 330.688949 143.224679 \nL 331.908844 143.105649 \nL 332.518791 143.271675 \nL 334.348632 143.094412 \nL 336.178474 142.253236 \nL 336.788421 142.196961 \nL 337.398368 142.583167 \nL 338.008315 142.526406 \nL 338.618263 142.249775 \nL 339.22821 141.535095 \nL 341.667998 142.191099 \nL 342.277946 142.136076 \nL 342.887893 142.298052 \nL 344.107787 141.75666 \nL 345.327681 142.079419 \nL 345.937629 142.454137 \nL 348.377417 143.086724 \nL 348.987365 143.031083 \nL 350.207259 142.498114 \nL 351.427153 142.810858 \nL 352.647048 142.702153 \nL 353.256995 143.066046 \nL 353.866942 143.011392 \nL 355.696783 142.227153 \nL 356.306731 142.381485 \nL 356.916678 142.328819 \nL 357.526625 141.864555 \nL 358.136572 142.018667 \nL 358.746519 141.967075 \nL 359.356466 141.50645 \nL 359.966414 141.45614 \nL 360.576361 141.609824 \nL 361.186308 141.356138 \nL 363.626097 141.965411 \nL 364.236044 142.317464 \nL 364.845991 142.467215 \nL 365.455938 142.415874 \nL 366.065885 142.764878 \nL 366.675832 142.713154 \nL 367.28578 142.462355 \nL 367.895727 142.610296 \nL 368.505674 142.956118 \nL 369.115621 142.508276 \nL 369.725568 142.259893 \nL 369.725568 142.259893 \n\" clip-path=\"url(#p98521e957c)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 50.14375 149.771461 \nL 384.94375 149.771461 \n\" clip-path=\"url(#p98521e957c)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #000000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 251.82 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 251.82 \nL 384.94375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 251.82 \nL 384.94375 251.82 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 384.94375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 302.089063 103.26875 \nL 377.94375 103.26875 \nQ 379.94375 103.26875 379.94375 101.26875 \nL 379.94375 14.2 \nQ 379.94375 12.2 377.94375 12.2 \nL 302.089063 12.2 \nQ 300.089063 12.2 300.089063 14.2 \nL 300.089063 101.26875 \nQ 300.089063 103.26875 302.089063 103.26875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 304.089063 20.298437 \nL 314.089063 20.298437 \nL 324.089063 20.298437 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- P(die=1) -->\n     <g transform=\"translate(332.089063 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 304.089063 34.976562 \nL 314.089063 34.976562 \nL 324.089063 34.976562 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- P(die=2) -->\n     <g transform=\"translate(332.089063 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-32\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 304.089063 49.654687 \nL 314.089063 49.654687 \nL 324.089063 49.654687 \n\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_16\">\n     <!-- P(die=3) -->\n     <g transform=\"translate(332.089063 53.154687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-33\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 304.089063 64.332812 \nL 314.089063 64.332812 \nL 324.089063 64.332812 \n\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- P(die=4) -->\n     <g transform=\"translate(332.089063 67.832812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-34\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 304.089063 79.010937 \nL 314.089063 79.010937 \nL 324.089063 79.010937 \n\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- P(die=5) -->\n     <g transform=\"translate(332.089063 82.510937)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-35\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 304.089063 93.689062 \nL 314.089063 93.689062 \nL 324.089063 93.689062 \n\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- P(die=6) -->\n     <g transform=\"translate(332.089063 97.189062)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-36\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p98521e957c\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"334.8\" height=\"244.62\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 432x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = tfp.distributions.Multinomial(10, fair_probs).sample(500)\n",
    "cum_counts = tf.cumsum(counts, axis=0)\n",
    "estimates = cum_counts / tf.reduce_sum(cum_counts, axis=1, keepdims=True)\n",
    "\n",
    "d2l.set_figsize((6, 4.5))\n",
    "for i in range(6):\n",
    "    d2l.plt.plot(estimates[:, i].numpy(),\n",
    "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
    "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axioms of Probability Theory\n",
    "When dealing with the rolls of die, the set S = {1,2,3,4,5,6} - the sample space or outcome space, where each element is an outcome.\n",
    "An event is a set of outcomes from a given sample space. For instance, seeing {5} or seeing an odd number {1,3,5} are both valid events of rolling a die.\n",
    "If the outcome of a random experience is an event A, then event A has occured. \n",
    "\n",
    "Formally, probability can be thought of as a function that maps a set to a real value.\n",
    "The probability of an event A in the given sample space S, denoted P(A) statisfies the following properties: (Kolmogorov, 1933)\n",
    "1. For any event A, its probability is never negative\n",
    "2. Probability of the entire sample space is 1, P(S)=1\n",
    "3. For any countable sequence of events A1,A2,... that are mutually exclusive, the probability that any happens is equal to the sum of their individual probabilities.\n",
    "   $$ \n",
    "   A_{i} \\cap A_{j} = 0 \\space and \\space P(U^{\\infty}_{i01} A_{i})= \\sum^{\\infty} _{i=1} P(A_{i})\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Variables\n",
    "A random variable can be pretty much any quantity and is not deterministic. It could take one value among a set of possibilities in a random experiment.\n",
    "\n",
    "For a compact notation:\n",
    "* P(X) - distribution over random variable X, the distribution tells us the probability that X takes any value.\n",
    "* P(a) - probability that a random variable will take value a.\n",
    "\n",
    "Important to state difference between continuous and discrete variables. For continuous variables, it makes more sense to count if the value falls in the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Multiple Random Variable\n",
    "\n",
    "### Joint Probability\n",
    "P(A=a, B= b). Given any values a and b, the joint probability considers: whatr is the probability  that A=a and B=b simultaneously.\n",
    "Note that for any values a and b,\n",
    "\n",
    "$$\n",
    "P(A=a, B=b) \\leq P(A=a)\n",
    "$$\n",
    "\n",
    "### Conditional Probability\n",
    "Shows the probability of B=b, provided that A=a has occured.\n",
    "$$\n",
    "0 \\leq \\frac{P(A=a,B=b)}{P(A=a)} \\leq 1\n",
    "$$\n",
    "\n",
    "### Bayes´ Theorem\n",
    "Using the definition of conditional probability, the celebrated equation - Bayes' Theorem can be derived.\n",
    "Note:\n",
    "* P(A|B) - conditional distribution\n",
    "* P(A,B) - joint distribution\n",
    "$$\n",
    "P(A|B) = \\frac {P(B|A)P(A)}{P(B)}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginalisation\n",
    "Bayes' theorem is useful when we want to infer one thing from the other (cause and effect).\n",
    "But we know the properties in the reverse direction. Them we need one important operation, marginalisation, determining P(B) from P(A,B).\n",
    "The probability of P(B) amounts to accounting for all possible choices of A and aggregating the joint probability over all of them. Known as a sum rule. The probability of distribution as a result of marginalisation is called a marginal probabiity or marginal distribution.\n",
    "$$\n",
    "P(B) = \\sum_{A} P(A,B)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence\n",
    "Two variables being independent means that the occurence of one event A does not reveal any information about the occurence of an event B.\n",
    "In this case, P(B|A) = P(B). And in mathematical notation, \n",
    "\n",
    "$$\n",
    "A \\perp B \\space or \\space A\\perp B |C\n",
    "$$\n",
    "Two random variables are independent if and only if their joint distribution is the product of their individual distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(dir(tf.random))\n",
    "\n",
    "help(tf.ones)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44a9fa3f3b6503a81a36a9be1bf5a0da396980bea9c30d12f51b48ce89a3e8f7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
